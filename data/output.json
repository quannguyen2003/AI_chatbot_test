{
    "ML001": {
        "content": "Nếu như con người có kiểu học “nước đến chân mới nhảy”, thì trong Machine Learning cũng có một thuật toán như vậy.\nTrong trang này:\n1. Giới thiệu\nMột câu chuyện vui\nK-nearest neighbor\nKhoảng cách trong không gian vector\n2. Phân tích toán học\n3. Ví dụ trên Python\nBộ cơ sở dữ liệu Iris (Iris flower dataset).\nThí nghiệm\nTách training và test sets\nPhương pháp đánh giá (evaluation method)\nĐánh trọng số cho các điểm lân cận\n4. Thảo luận\nKNN cho Regression\nChuẩn hóa dữ liệu\nSử dụng các phép đo khoảng cách khác nhau\nƯu điểm của KNN\nNhược điểm của KNN\nTăng tốc cho KNN\nTry this yourself\nSource code\n5. Tài liệu tham khảo\n1. Giới thiệu\nMột câu chuyện vui\nCó một anh bạn chuẩn bị đến ngày thi cuối kỳ. Vì môn này được mở tài liệu khi thi nên anh ta không chịu ôn tập để hiểu ý nghĩa của từng bài học và mối liên hệ giữa các bài. Thay vào đó, anh thu thập tất cả các tài liệu trên lớp, bao gồm ghi chép bài giảng (lecture notes), các slides và bài tập về nhà + lời giải. Để cho chắc, anh ta ra thư viện và các quán Photocopy quanh trường mua hết tất cả các loại tài liệu liên quan (khá khen cho cậu này chịu khó tìm kiếm tài liệu). Cuối cùng, anh bạn của chúng ta thu thập được một chồng cao tài liệu để mang vào phòng thi.\nVào ngày thi, anh tự tin mang chồng tài liệu vào phòng thi. Aha, đề này ít nhất mình phải được 8 điểm. Câu 1 giống hệt bài giảng trên lớp. Câu 2 giống hệt đề thi năm ngoái mà lời giải có trong tập tài liệu mua ở quán Photocopy. Câu 3 gần giống với bài tập về nhà. Câu 4 trắc nghiệm thậm chí cậu nhớ chính xác ba tài liệu có ghi đáp án. Câu cuối cùng, 1 câu khó nhưng anh đã từng nhìn thấy, chỉ là không nhớ ở đâu thôi.\nKết quả cuối cùng, cậu ta được 4 điểm, vừa đủ điểm qua môn. Cậu làm chính xác câu 1 vì tìm được ngay trong tập ghi chú bài giảng. Câu 2 cũng tìm được đáp án nhưng lời giải của quán Photocopy sai! Câu ba thấy gần giống bài về nhà, chỉ khác mỗi một số thôi, cậu cho kết quả giống như thế luôn, vậy mà không được điểm nào. Câu 4 thì tìm được cả 3 tài liệu nhưng có hai trong đó cho đáp án A, cái còn lại cho B. Cậu chọn A và được điểm. Câu 5 thì không làm được dù còn tới 20 phút, vì tìm mãi chẳng thấy đáp án đâu - nhiều tài liệu quá cũng mệt!!\nKhông phải ngẫu nhiên mà tôi dành ra ba đoạn văn để kể về chuyện học hành của anh chàng kia. Hôm nay tôi xin trình bày về một phương pháp trong Machine Learning, được gọi là K-nearest neighbor (hay KNN), một thuật toán được xếp vào loại lazy (machine) learning (máy lười học). Thuật toán này khá giống với cách học/thi của anh bạn kém may mắn kia.\nK-nearest neighbor\nK-nearest neighbor là một trong những thuật toán supervised-learning đơn giản nhất (mà hiệu quả trong một vài trường hợp) trong Machine Learning. Khi training, thuật toán này không học một điều gì từ dữ liệu training (đây cũng là lý do thuật toán này được xếp vào loại lazy learning), mọi tính toán được thực hiện khi nó cần dự đoán kết quả của dữ liệu mới. K-nearest neighbor có thể áp dụng được vào cả hai loại của bài toán Supervised learning là Classification và Regression. KNN còn được gọi là một thuật toán Instance-based hay Memory-based learning.\nCó một vài khái niệm tương ứng người-máy như sau:\nNgôn ngữ người\nNgôn ngữ Máy Học\nin Machine Learning\nCâu hỏi\nĐiểm dữ liệu\nData point\nĐáp án\nĐầu ra, nhãn\nOutput, Label\nÔn thi\nHuấn luyện\nTraining\nTập tài liệu mang vào phòng thi\nTập dữ liệu tập huấn\nTraining set\nĐề thi\nTập dữ liểu kiểm thử\nTest set\nCâu hỏi trong dề thi\nDữ liệu kiểm thử\nTest data point\nCâu hỏi có đáp án sai\nNhiễu\nNoise, Outlier\nCâu hỏi gần giống\nĐiểm dữ liệu gần nhất\nNearest Neighbor\nVới KNN, trong bài toán Classification, label của một điểm dữ liệu mới (hay kết quả của câu hỏi trong bài thi) được suy ra trực tiếp từ K điểm dữ liệu gần nhất trong training set. Label của một test data có thể được quyết định bằng major voting (bầu chọn theo số phiếu) giữa các điểm gần nhất, hoặc nó có thể được suy ra bằng cách đánh trọng số khác nhau cho mỗi trong các điểm gần nhất đó rồi suy ra label. Chi tiết sẽ được nêu trong phần tiếp theo.\nTrong bài toán Regresssion, đầu ra của một điểm dữ liệu sẽ bằng chính đầu ra của điểm dữ liệu đã biết gần nhất (trong trường hợp K=1), hoặc là trung bình có trọng số của đầu ra của những điểm gần nhất, hoặc bằng một mối quan hệ dựa trên khoảng cách tới các điểm gần nhất đó.\nMột cách ngắn gọn, KNN là thuật toán đi tìm đầu ra của một điểm dữ liệu mới bằng cách chỉ dựa trên thông tin của K điểm dữ liệu trong training set gần nó nhất (K-lân cận), không quan tâm đến việc có một vài điểm dữ liệu trong những điểm gần nhất này là nhiễu. Hình dưới đây là một ví dụ về KNN trong classification với K = 1.\nBản đồ của 1NN (Nguồn: Wikipedia)\nVí dụ trên đây là bài toán Classification với 3 classes: Đỏ, Lam, Lục. Mỗi điểm dữ liệu mới (test data point) sẽ được gán label theo màu của điểm mà nó thuộc về. Trong hình này, có một vài vùng nhỏ xem lẫn vào các vùng lớn hơn khác màu. Ví dụ có một điểm màu Lục ở gần góc 11 giờ nằm giữa hai vùng lớn với nhiều dữ liệu màu Đỏ và Lam. Điểm này rất có thể là nhiễu. Dẫn đến nếu dữ liệu test rơi vào vùng này sẽ có nhiều khả năng cho kết quả không chính xác.\nKhoảng cách trong không gian vector\nTrong không gian một chiều, khoảng cách giữa hai điểm là trị tuyệt đối giữa hiệu giá trị của hai điểm đó. Trong không gian nhiều chiều, khoảng cách giữa hai điểm có thể được định nghĩa bằng nhiều hàm số khác nhau, trong đó độ dài đường thằng nổi hai điểm chỉ là một trường hợp đặc biệt trong đó. Nhiều thông tin bổ ích (cho Machine Learning) có thể được tìm thấy tại Norms (chuẩn) của vector trong tab Math.\n2. Phân tích toán học\nThuật toán KNN rất dễ hiểu nên sẽ phần “Phân tích toán học” này sẽ chỉ có 3 câu. Tôi trực tiếp đi vào các ví dụ. Có một điều đáng lưu ý là KNN phải nhớ tất cả các điểm dữ liệu training, việc này không được lợi về cả bộ nhớ và thời gian tính toán - giống như khi cậu bạn của chúng ta không tìm được câu trả lời cho câu hỏi cuối cùng.\n3. Ví dụ trên Python\nBộ cơ sở dữ liệu Iris (Iris flower dataset).\nIris flower dataset là một bộ dữ liệu nhỏ (nhỏ hơn rất nhiều so với MNIST. Bộ dữ liệu này bao gồm thông tin của ba loại hoa Iris (một loài hoa lan) khác nhau: Iris setosa, Iris virginica và Iris versicolor. Mỗi loại có 50 bông hoa được đo với dữ liệu là 4 thông tin: chiều dài, chiều rộng đài hoa (sepal), và chiều dài, chiều rộng cánh hoa (petal). Dưới đây là ví dụ về hình ảnh của ba loại hoa. (Chú ý, đây không phải là bộ cơ sở dữ liệu ảnh như MNIST, mỗi điểm dữ liệu trong tập này chỉ là một vector 4 chiều).\nVí dụ về Iris flower dataset (Nguồn: Wikipedia)\nBộ dữ liệu nhỏ này thường được sử dụng trong nhiều thuật toán Machine Learning trong các lớp học. Tôi sẽ giải thích lý do không chọn MNIST vào phần sau.\nThí nghiệm\nTrong phần này, chúng ta sẽ tách 150 dữ liệu trong Iris flower dataset ra thành 2 phần, gọi là training set và test set. Thuật toán KNN sẽ dựa vào trông tin ở training set để dự đoán xem mỗi dữ liệu trong test set tương ứng với loại hoa nào. Dữ liệu được dự đoán này sẽ được đối chiếu với loại hoa thật của mỗi dữ liệu trong test set để đánh giá hiệu quả của KNN.\nTrước tiên, chúng ta cần khai báo vài thư viện.\nIris flower dataset có sẵn trong thư viện scikit-learn.\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import neighbors, datasets\nTiếp theo, chúng ta load dữ liệu và hiện thị vài dữ liệu mẫu. Các class được gán nhãn là 0, 1, và 2.\niris = datasets.load_iris()\niris_X = iris.data\niris_y = iris.target\nprint 'Number of classes: %d' %len(np.unique(iris_y))\nprint 'Number of data points: %d' %len(iris_y)\nX0 = iris_X[iris_y == 0,:]\nprint '\\nSamples from class 0:\\n', X0[:5,:]\nX1 = iris_X[iris_y == 1,:]\nprint '\\nSamples from class 1:\\n', X1[:5,:]\nX2 = iris_X[iris_y == 2,:]\nprint '\\nSamples from class 2:\\n', X2[:5,:]\nNumber of classes: 3\nNumber of data points: 150\nSamples from class 0:\n[[ 5.1  3.5  1.4  0.2]\n[ 4.9  3.   1.4  0.2]\n[ 4.7  3.2  1.3  0.2]\n[ 4.6  3.1  1.5  0.2]\n[ 5.   3.6  1.4  0.2]]\nSamples from class 1:\n[[ 7.   3.2  4.7  1.4]\n[ 6.4  3.2  4.5  1.5]\n[ 6.9  3.1  4.9  1.5]\n[ 5.5  2.3  4.   1.3]\n[ 6.5  2.8  4.6  1.5]]\nSamples from class 2:\n[[ 6.3  3.3  6.   2.5]\n[ 5.8  2.7  5.1  1.9]\n[ 7.1  3.   5.9  2.1]\n[ 6.3  2.9  5.6  1.8]\n[ 6.5  3.   5.8  2.2]]\nNếu nhìn vào vài dữ liệu mẫu, chúng ta thấy rằng hai cột cuối mang khá nhiều thông tin giúp chúng ta có thể  phân biệt được chúng. Chúng ta dự đoán rằng kết quả classification cho cơ sở dữ liệu này sẽ tương đối cao.\nTách training và test sets\nGiả sử chúng ta muốn dùng 50 điểm dữ liệu cho test set, 100 điểm còn lại cho training set. Scikit-learn có một hàm số cho phép chúng ta ngẫu nhiên lựa chọn các điểm này, như sau:\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\niris_X, iris_y, test_size=50)\nprint \"Training size: %d\" %len(y_train)\nprint \"Test size    : %d\" %len(y_test)\nTraining size: 100\nTest size    : 50\nSau đây, tôi trước hết xét trường hợp đơn giản K = 1, tức là với mỗi điểm test data, ta chỉ xét 1 điểm training data gần nhất và lấy label của điểm đó để dự đoán cho điểm test này.\nclf = neighbors.KNeighborsClassifier(n_neighbors = 1, p = 2)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nprint \"Print results for 20 test data points:\"\nprint \"Predicted labels: \", y_pred[20:40]\nprint \"Ground truth    : \", y_test[20:40]\nPrint results for first 20 test data points:\nPredicted labels:  [2 1 2 2 1 2 2 0 2 0 2 0 1 0 0 2 2 0 2 0]\nGround truth    :  [2 1 2 2 1 2 2 0 2 0 1 0 1 0 0 2 1 0 2 0]\nKết quả cho thấy label dự đoán gần giống với label thật của test data, chỉ có 2 điểm trong số 20 điểm được hiển thị có kết quả sai lệch. Ở đây chúng ta làm quen với khái niệm mới: ground truth. Một cách đơn giản, ground truth chính là nhãn/label/đầu ra thực sự của các điểm trong test data. Khái niệm này được dùng nhiều trong Machine Learning, hy vọng lần tới các bạn gặp thì sẽ nhớ ngay nó là gì.\nPhương pháp đánh giá (evaluation method)\nĐể đánh giá độ chính xác của thuật toán KNN classifier này, chúng ta xem xem có bao nhiêu điểm trong test data được dự đoán đúng. Lấy số lượng này chia cho tổng số lượng trong tập test data sẽ ra độ chính xác. Scikit-learn cung cấp hàm số accuracy_score để thực hiện công việc này.\nfrom sklearn.metrics import accuracy_score\nprint \"Accuracy of 1NN: %.2f %%\" %(100*accuracy_score(y_test, y_pred))\nAccuracy of 1NN: 94.00 %\n1NN đã cho chúng ta kết quả là 94%, không tệ! Chú ý rằng đây là một cơ sở dữ liệu dễ vì chỉ với dữ liệu ở hai cột cuối cùng, chúng ta đã có thể suy ra quy luật. Trong ví dụ này, tôi sử dụng p = 2 nghĩa là khoảng cách ở đây được tính là khoảng cách theo norm 2. Các bạn cũng có thể thử bằng cách thay p = 1 cho norm 1, hoặc các gía trị p khác cho norm khác. (Xem thêm sklearn.neighbors.KNeighborsClassifier)\nNhận thấy rằng chỉ xét 1 điểm gần nhất có thể dẫn đến kết quả sai nếu điểm đó là nhiễu. Một cách có thể làm tăng độ chính xác là tăng số lượng điểm lân cận lên, ví dụ 10 điểm, và xem xem trong 10 điểm gần nhất, class nào chiếm đa số thì dự đoán kết quả là class đó. Kỹ thuật dựa vào đa số này được gọi là major voting.\nclf = neighbors.KNeighborsClassifier(n_neighbors = 10, p = 2)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nprint \"Accuracy of 10NN with major voting: %.2f %%\" %(100*accuracy_score(y_test, y_pred))\nAccuracy of 10NN with major voting: 98.00 %\nKết quả đã tăng lên 98%, rất tốt!\nĐánh trọng số cho các điểm lân cận\nLà một kẻ tham lam, tôi chưa muốn dừng kết quả ở đây vì thấy rằng mình vẫn có thể cải thiện được. Trong kỹ thuật major voting bên trên, mỗi trong 10 điểm gần nhất được coi là có vai trò như nhau và giá trị lá phiếu của mỗi điểm này là như nhau. Tôi cho rằng như thế là không công bằng, vì rõ ràng rằng những điểm gần hơn nên có trọng số cao hơn (càng thân cận thì càng tin tưởng). Vậy nên tôi sẽ đánh trọng số khác nhau cho mỗi trong 10 điểm gần nhất này. Cách đánh trọng số phải thoải mãn điều kiện là một điểm càng gần điểm test data thì phải được đánh trọng số càng cao (tin tưởng hơn). Cách đơn giản nhất là lấy nghịch đảo của khoảng cách này. (Trong trường hợp test data trùng với 1 điểm dữ liệu trong training data, tức khoảng cách bằng 0, ta lấy luôn label của điểm training data).\nScikit-learn giúp chúng ta đơn giản hóa việc này bằng cách gán gía trị weights = 'distance'. (Giá trị mặc định của weights là 'uniform', tương ứng với việc coi tất cả các điểm lân cận có giá trị như nhau như ở trên).\nclf = neighbors.KNeighborsClassifier(n_neighbors = 10, p = 2, weights = 'distance')\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nprint \"Accuracy of 10NN (1/distance weights): %.2f %%\" %(100*accuracy_score(y_test, y_pred))\nAccuracy of 10NN (1/distance weights): 100.00 %\nAha, 100%.\nChú ý: Ngoài 2 phương pháp đánh trọng số weights = 'uniform' và weights = 'distance' ở trên, scikit-learn còn cung cấp cho chúng ta một cách để đánh trọng số một cách tùy chọn. Ví dụ, một cách đánh trọng số phổ biến khác trong Machine Learning là:\n\\[\nw_i = \\exp \\left( \\frac{-||\\mathbf{x} - \\mathbf{x}_i||_2^2}{\\sigma^2} \\right)\n\\]\ntrong đó \\(\\mathbf{x}\\) là test data, \\(\\mathbf{x}_i\\) là một điểm trong K-lân cận của \\(\\mathbf{x}\\), \\(w_i\\) là trọng số của điểm đó (ứng với điểm dữ liệu đang xét \\(\\mathbf{x}\\)), \\(\\sigma\\) là một số dương. Nhận thấy rằng hàm số này cũng thỏa mãn điều kiện: điểm càng gần \\(\\mathbf{x}\\) thì trọng số càng cao (cao nhất bằng 1). Với hàm số này, chúng ta có thể lập trình như sau:\ndef myweight(distances):\nsigma2 = .5 # we can change this number\nreturn np.exp(-distances**2/sigma2)\nclf = neighbors.KNeighborsClassifier(n_neighbors = 10, p = 2, weights = myweight)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nprint \"Accuracy of 10NN (customized weights): %.2f %%\" %(100*accuracy_score(y_test, y_pred))\nAccuracy of 10NN (customized weights): 98.00 %\nTrong trường hợp này, kết quả tương đương với kỹ thuật major voting. Để đánh giá chính xác hơn kết quả của KNN với K khác nhau, cách định nghĩa khoảng cách khác nhau và cách đánh trọng số khác nhau, chúng ta cần thực hiện quá trình trên với nhiều cách chia dữ liệu training và test khác nhau rồi lấy kết quả trung bình, vì rất có thể dữ liệu phân chia trong 1 trường hợp cụ thể là rất tốt hoặc rất xấu (bias). Đây cũng là cách thường được dùng khi đánh giá hiệu năng của một thuật toán cụ thể nào đó.\n4. Thảo luận\nKNN cho Regression\nVới bài toán Regression, chúng ta cũng hoàn toàn có thể sử dụng phương pháp tương tự: ước lượng đầu ra dựa trên đầu ra và khoảng cách của các điểm trong K-lân cận. Việc ước lượng như thế nào các bạn có thể tự định nghĩa tùy vào từng bài toán.\nKNN cho bài toán Regression  (Nguồn: Nearest Neighbors regression)\nChuẩn hóa dữ liệu\nKhi có một thuộc tính trong dữ liệu (hay phần tử trong vector) lớn hơn các thuộc tính khác rất nhiều (ví dụ thay vì đo bằng cm thì một kết quả lại tính bằng mm), khoảng cách giữa các điểm sẽ phụ thuộc vào thuộc tính này rất nhiều. Để có được kết quả chính xác hơn, một kỹ thuật thường được dùng là Data Normalization (chuẩn hóa dữ liệu) để đưa các thuộc tính có đơn vị đo khác nhau về cùng một khoảng giá trị, thường là từ 0 đến 1, trước khi thực hiện KNN. Có nhiều kỹ thuật chuẩn hóa khác nhau, các bạn sẽ được thấy khi tiếp tục theo dõi Blog này. Các kỹ thuật chuẩn hóa được áp dụng với không chỉ KNN mà còn với hầu hết các thuật toán khác.\nSử dụng các phép đo khoảng cách khác nhau\nNgoài norm 1 và norm 2 tôi giới thiệu trong bài này, còn rất nhiều các khoảng cách khác nhau có thể được dùng. Một ví dụ đơn giản là đếm số lượng thuộc tính khác nhau giữa hai điểm dữ liệu. Số này càng nhỏ thì hai điểm càng gần nhau. Đây chính là giả chuẩn 0 mà tôi đã giới thiệu trong Tab Math.\nƯu điểm của KNN\nĐộ phức tạp tính toán của quá trình training là bằng 0.\nViệc dự đoán kết quả của dữ liệu mới rất đơn giản.\nKhông cần giả sử gì về phân phối của các class.\nNhược điểm của KNN\nKNN rất nhạy cảm với nhiễu khi K nhỏ.\nNhư đã nói, KNN là một thuật toán mà mọi tính toán đều nằm ở khâu test. Trong đó việc tính khoảng cách tới từng điểm dữ liệu trong training set sẽ tốn rất nhiều thời gian, đặc biệt là với các cơ sở dữ liệu có số chiều lớn và có nhiều điểm dữ liệu. Với K càng lớn thì độ phức tạp cũng sẽ tăng lên. Ngoài ra, việc lưu toàn bộ dữ liệu trong bộ nhớ cũng ảnh hưởng tới hiệu năng của KNN.\nTăng tốc cho KNN\nNgoài việc tính toán khoảng cách từ một điểm test data đến tất cả các điểm trong traing set (Brute Force), có một số thuật toán khác giúp tăng tốc việc tìm kiếm này. Bạn đọc có thẻ tìm kiếm thêm với hai từ khóa: K-D Tree và Ball Tree. Tôi xin dành phần này cho độc giả tự tìm hiểu, và sẽ quay lại nếu có dịp. Chúng ta vẫn còn những thuật toán quan trọng hơn khác cần nhiều sự quan tâm hơn.\nTry this yourself\nTôi có viết một đoạn code ngắn để thực hiện việc Classification cho cơ sở dữ liệu MNIST. Các bạn hãy download toàn bộ bộ dữ liệu này về vì sau này chúng ta còn dùng nhiều, chạy thử, comment kết quả và nhận xét của các bạn vào phần comment bên dưới. Để trả lời cho câu hỏi vì sao tôi không chọn cơ sở dữ liệu này làm ví dụ, bạn đọc có thể tự tìm ra đáp án khi chạy xong đoạn code này.\nEnjoy!\n# %reset\nimport numpy as np\nfrom mnist import MNIST # require `pip install python-mnist`\n# https://pypi.python.org/pypi/python-mnist/\nimport matplotlib.pyplot as plt\nfrom sklearn import neighbors\nfrom sklearn.metrics import accuracy_score\nimport time\n# you need to download the MNIST dataset first\n# at: http://yann.lecun.com/exdb/mnist/\nmndata = MNIST('../MNIST/') # path to your MNIST folder\nmndata.load_testing()\nmndata.load_training()\nX_test = mndata.test_images\nX_train = mndata.train_images\ny_test = np.asarray(mndata.test_labels)\ny_train = np.asarray(mndata.train_labels)\nstart_time = time.time()\nclf = neighbors.KNeighborsClassifier(n_neighbors = 1, p = 2)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nend_time = time.time()\nprint \"Accuracy of 1NN for MNIST: %.2f %%\" %(100*accuracy_score(y_test, y_pred))\nprint \"Running time: %.2f (s)\" % (end_time - start_time)\nSource code\niPython Notebook cho bài này có thể download tại đây.\n5. Tài liệu tham khảo\nsklearn.neighbors.NearestNeighbors\nsklearn.model_selection.train_test_split\nTutorial To Implement k-Nearest Neighbors in Python From Scratch",
        "summary": "Thuật toán K-nearest neighbor (KNN) là một phương pháp học máy đơn giản, được ví như cách học \"nước đến chân mới nhảy\" của con người, khi nó không học từ dữ liệu huấn luyện mà chỉ dựa vào các điểm dữ liệu gần nhất để dự đoán kết quả cho dữ liệu mới. KNN có thể được áp dụng cho cả bài toán phân loại (classification) và hồi quy (regression), với ưu điểm là dễ hiểu và không cần giả sử gì về phân phối của dữ liệu. Tuy nhiên, KNN cũng có nhược điểm là nhạy cảm với nhiễu và tốn nhiều thời gian tính toán, đặc biệt khi số lượng dữ liệu lớn và số chiều cao. \n",
        "status": true
    },
    "ML002": {
        "content": "Trong trang này:1. Giới thiệuNhắc lại hai mô hình tuyến tínhMột ví dụ nhỏMô hình Logistic RegressionSigmoid function2. Hàm mất mát và phương pháp tối ưuXây dựng hàm mất mátTối ưu hàm mất mátCông thức cập nhật cho logistic sigmoid regression3. Ví dụ với PythonVí dụ với dữ liệu 1 chiềuCác hàm cần thiết cho logistic sigmoid regressionVí dụ với dữ liệu 2 chiều4. Một vài tính chất của Logistic RegressionLogistic Regression thực ra được sử dụng nhiều trong các bài toán Classification.Boundary tạo bởi Logistic Regression có dạng tuyến tính5. Thảo luận6. Tài liệu tham khảo1. Giới thiệuNhắc lại hai mô hình tuyến tínhHai mô hình tuyến tính (linear models)Linear RegressionvàPerceptron Learning Algorithm(PLA) chúng ta đã biết đều có chung một dạng:\n\\[\ny = f(\\mathbf{w}^T\\mathbf{x})\n\\]trong đó \\(f()\\) được gọi làactivation function, và \\(\\mathbf{x}\\) được hiểu là dữ liệu mở rộng với \\(x_0 = 1\\) được thêm vào để thuận tiện cho việc tính toán. Với linear regression thì \\(f(s) = s\\), với PLA thì \\(f(s) = \\text{sgn}(s)\\). Trong linear regression, tích vô hướng \\(\\mathbf{w}^T\\mathbf{x}\\) được trực tiếp sử dụng để dự đoán output \\(y\\), loại này phù hợp nếu chúng ta cần dự đoán một giá trị thực của đầu ra không bị chặn trên và dưới. Trong PLA, đầu ra chỉ nhận một trong hai giá trị \\(1\\) hoặc \\(-1 \\), phù hợp với các bài toánbinary classification.Trong bài này, tôi sẽ giới thiệu mô hình thứ ba với một activation khác, được sử dụng cho các bài toánflexiblehơn. Trong dạng này, đầu ra có thể được thể hiện dưới dạng xác suất (probability). Ví dụ: xác suất thi đỗ nếu biết thời gian ôn thi, xác suất ngày mai có mưa dựa trên những thông tin đo được trong ngày hôm nay,… Mô hình mới này của chúng ta có tên làlogistic regression. Mô hình này giống với linear regression ở khía cạnh đầu ra là số thực, và giống với PLA ở việc đầu ra bị chặn (trong đoạn \\([0, 1]\\)). Mặc dù trong tên có chứa từregression, logistic regression thường được sử dụng nhiều hơn cho các bài toán classification.Một ví dụ nhỏTôi xin được sử dụngmột ví dụ trên Wikipedia:Một nhóm 20 sinh viên dành thời gian trong khoảng từ 0 đến 6 giờ cho việc ôn thi. Thời gian ôn thi này ảnh hưởng đến xác suất sinh viên vượt qua kỳ thi như thế nào?Kết quả thu được như sau:HoursPassHoursPass.502.751.75030103.2511.2503.501.50411.7504.2511.7514.51204.7512.251512.505.51Mặc dù có một chútbất côngkhi học 3.5 giờ thì trượt, còn học 1.75 giờ thì lại đỗ, nhìn chung, học càng nhiều thì khả năng đỗ càng cao. PLA không thể áp dụng được cho bài toán này vì không thể nói một người học bao nhiêu giờ thì 100% trượt hay đỗ, và thực tế là dữ liệu này cũng khônglinearly separable(điệu kiện để PLA có thể làm việc). Chú ý rằng các điểm màu đỏ và xanh được vẽ ở hai tung độ khác nhau để tiện cho việc minh họa. Các điểm này được vẽ dùng cả dữ liệu đầu vào \\(\\mathbf{x}\\) và đầu ra \\(y). Khi ta nóilinearly seperablelà khi ta chỉ dùng dữ liệu đầu vào \\(\\mathbf{x}\\).Chúng ta biểu diễn các điểm này trên đồ thị để thấy rõ hơn:Hình 1: Ví dụ về kết quả thi dựa trên số giờ ôn tập.Nhận thấy rằng cả linear regression và PLA đều không phù hợp với bài toán này, chúng ta cần một mô hìnhflexiblehơn.Mô hình Logistic RegressionĐầu ra dự đoán của:Linear Regression: \n\\[\nf(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{x}\n\\]PLA:\n\\[\nf(\\mathbf{x}) = \\text{sgn}(\\mathbf{w}^T\\mathbf{x})\n\\]Đầu ra dự đoán của logistic regression thường được viết chung dưới dạng:\n\\[\nf(\\mathbf{x}) = \\theta(\\mathbf{w}^T\\mathbf{x})\n\\]Trong đó \\(\\theta\\) được gọi là logistic function. Một số activation cho mô hình tuyến tính được cho trong hình dưới đây:Hình 2: Các activation function khác nhau.Đường màu vàng biểu diễn linear regression. Đường này không bị chặn nên không phù hợp cho bài toán này. Có mộttricknhỏ để đưa nó về dạng bị chặn:cắtphần nhỏ hơn 0 bằng cách cho chúng bằng 0,cắtcác phần lớn hơn 1 bằng cách cho chúng bằng 1. Sau đó lấy điểm trên đường thẳng này có tung độ bằng 0.5 làm điểm phân chia haiclass, đây cũng không phải là một lựa chọn tốt. Giả sử có thêm vài bạnsinh viên tiêu biểuôn tập đến 20 giờ và, tất nhiên, thi đỗ. Khi áp dụng mô hình linear regression như hình dưới đây và lấy mốc 0.5 để phân lớp, toàn bộ sinh viên thi trượt vẫn được dự đoán là trượt, nhưng rất nhiều sinh viên thi đỗ cũng được dự đoán là trượt (nếu ta coi điểm x màu xanh lục làngưỡng cứngđể đưa ra kết luận). Rõ ràng đây là một mô hình không tốt. Anh chàng sinh viên tiêu biểu này đãkéo theorất nhiều bạn khác bị trượt.Hình 3: Tại sao Linear Regression không phù hợp?Đường màu đỏ (chỉ khác với activation function của PLA ở chỗ  hai class là 0 và 1 thay vì -1 và 1) cũng thuộc dạngngưỡng cứng(hard threshold). PLA không hoạt động trong bài toán này vì dữ liệu đã cho khônglinearly separable.Các đường màu xanh lam và xanh lục phù hợp với bài toán của chúng ta hơn. Chúng có một vài tính chất quan trọng sau:Là hàm số liên tục nhận giá trị thực, bị chặn trong khoảng \\((0, 1)\\).Nếu coi điểm có tung độ là 1/2 làm điểm phân chia thì các điểm càng xa điểm này về phía bên trái có giá trị càng gần 0. Ngược lại, các điểm càng xa điểm này về phía phải có giá trị càng gần 1. Điều nàykhớpvới nhận xét rằng học càng nhiều thì xác suất đỗ càng cao và ngược lại.Mượt(smooth) nên có đạo hàm mọi nơi, có thể được lợi trong việc tối ưu.Sigmoid functionTrong số các hàm số có 3 tính chất nói trên thì hàmsigmoid:\n\\[\nf(s) = \\frac{1}{1 + e^{-s}} \\triangleq \\sigma(s)\n\\]\nđược sử dụng nhiều nhất, vì nó bị chặn trong khoảng \\((0, 1)\\). Thêm nữa:\n\\[\n\\lim_{s \\rightarrow -\\infty}\\sigma(s) = 0; ~~ \\lim_{s \\rightarrow +\\infty}\\sigma(s) = 1 \n\\]\nĐặc biệt hơn nữa:\n\\[\n\\begin{eqnarray}\n\\sigma’(s) &=& \\frac{e^{-s}}{(1 + e^{-s})^2} \\newline\n&=& \\frac{1}{1 + e^{-s}} \\frac{e^{-s}}{1 + e^{-s}} \\newline\n&=& \\sigma(s)(1 - \\sigma(s))\n\\end{eqnarray}\n\\]\nCông thức đạo hàm đơn giản thế này giúp hàm số này được sử dụng rộng rãi. Ở phần sau, tôi sẽ lý giải việcngười ta đã tìm ra hàm số đặc biệt này như thế nào.Ngoài ra, hàmtanhcũng hay được sử dụng: \n\\[\n\\text{tanh}(s) = \\frac{e^{s} - e^{-s}}{e^s + e^{-s}}\n\\]Hàm số này nhận giá trị trong khoảng \\((-1, 1)\\) nhưng có thể dễ dàng đưa nó về khoảng \\((0, 1)\\). Bạn đọc có thể chứng minh được:\n\\[\n\\text{tanh}(s) = 2\\sigma(2s) - 1\n\\]2. Hàm mất mát và phương pháp tối ưuXây dựng hàm mất mátVới mô hình như trên (các activation màu xanh lam và lục), ta có thể giả sử rằng xác suất để một điểm dữ liệu \\(\\mathbf{x}\\) rơi vào class 1 là \\(f(\\mathbf{w}^T\\mathbf{x})\\) và rơi vào class 0 là \\(1 - f(\\mathbf{w}^T\\mathbf{x})\\). Với mô hình được giả sử như vậy, với các điểm dữ liệu training (đã biết đầu ra \\(y\\)), ta có thể viết như sau:\\[\n\\begin{eqnarray}\nP(y_i = 1 | \\mathbf{x}_i; \\mathbf{w}) &=& &f(\\mathbf{w}^T\\mathbf{x}_i)  ~~(1) \\newline\nP(y_i = 0 | \\mathbf{x}_i; \\mathbf{w}) &=& 1 - &f(\\mathbf{w}^T\\mathbf{x}_i)  ~~(2) \\newline\n\\end{eqnarray}\n\\]\ntrong đó \\( P(y_i = 1 | \\mathbf{x}_i; \\mathbf{w})\\) được hiểu là xác suất xảy ra sự kiện đầu ra \\(y_i = 1\\) khi biết tham số mô hình \\(\\mathbf{w}\\) và dữ liệu đầu vào \\(\\mathbf{x}_i\\). Bạn đọc có thể đọc thêmXác suất có điều kiện. Mục đích của chúng ta là tìm các hệ số \\(\\mathbf{w}\\) sao cho \\(f(\\mathbf{w}^T\\mathbf{x}_i)\\) càng gần với 1 càng tốt với các điểm dữ liệu thuộc class 1 và càng gần với 0 càng tốt với những điểm thuộc class 0.Ký hiệu \\(z_i = f(\\mathbf{w}^T\\mathbf{x}_i)\\) và viết gộp lại hai biểu thức bên trên ta có:\n\\[\nP(y_i| \\mathbf{x}_i; \\mathbf{w}) = z_i^{y_i}(1 - z_i)^{1- y_i}\n\\]Biểu thức này là tương đương với hai biểu thức \\((1)\\) và \\((2)\\) ở trên vì khi \\(y_i=1\\), phần thứ hai của vế phải sẽ triệt tiêu, khi \\(y_i = 0\\), phần thứ nhất sẽ bị triệt tiêu! Chúng ta muốn mô hình gần với dữ liệu đã cho nhất, tức xác suất này đạt giá trị cao nhất.Xét toàn bộ training set với \\(\\mathbf{X} = [\\mathbf{x}_1,\\mathbf{x}_2, \\dots, \\mathbf{x}_N] \\in \\mathbb{R}^{d \\times N}\\) và \\(\\mathbf{y} = [y_1, y_2, \\dots, y_N]\\), chúng ta cần tìm \\(\\mathbf{w}\\) để biểu thức sau đây đạt giá trị lớn nhất:\n\\[\nP(\\mathbf{y}|\\mathbf{X}; \\mathbf{w})\n\\]\nở đây, ta cũng ký hiệu \\(\\mathbf{X, y}\\) như cácbiến ngẫu nhiên(random variables). Nói cách khác:\n\\[\n\\mathbf{w} = \\arg\\max_{\\mathbf{w}} P(\\mathbf{y}|\\mathbf{X}; \\mathbf{w})\n\\]Bài toán tìm tham số để mô hình gần với dữ liệu nhất trên đây có tên gọi chung là bài toánmaximum likelihood estimationvới hàm số phía sau \\(\\arg\\max\\) được gọi làlikelihood function. Khi làm việc với các bài toán Machine Learning sử dụng các mô hình xác suất thống kê, chúng ta sẽ gặp lại các bài toán thuộc dạng này, hoặcmaximum a posteriori estimation, rất nhiều. Tôi sẽ dành 1 bài khác để nói về hai dạng bài toán này.Giả sử thêm rằng các điểm dữ liệu được sinh ra một cách ngẫu nhiên độc lập với nhau (independent), ta có thể viết:\n\\[\n\\begin{eqnarray}\nP(\\mathbf{y}|\\mathbf{X}; \\mathbf{w}) &=& \\prod_{i=1}^N P(y_i| \\mathbf{x}_i; \\mathbf{w}) \\newline\n&=& \\prod_{i=1}^N z_i^{y_i}(1 - z_i)^{1- y_i}\n\\end{eqnarray}\n\\]\nvới \\(\\prod\\) là ký hiệu của tích. Bạn đọc có thể muốn đọc thêm vềĐộc lập thống kê.Trực tiếp tối ưu hàm số này theo \\(\\mathbf{w}\\) nhìn qua không đơn giản! Hơn nữa, khi \\(N\\) lớn, tích của \\(N\\) số nhỏ hơn 1 có thể dẫn tới sai số trong tính toán (numerial error) vì tích là một số quá nhỏ. Một phương pháp thường được sử dụng đó là lấy logarit tự nhiên (cơ số \\(e\\)) củalikelihood functionbiến phép nhân thành phép cộng và để tránh việc số quá nhỏ. Sau đó lấy ngược dấu để được một hàm và coi nó là hàm mất mát. Lúc này bài toán tìm giá trị lớn nhất (maximum likelihood) trở thành bài toán tìm giá trị nhỏ nhất của hàm mất mát (hàm này còn được gọi là negative log likelihood):\n\\[\n\\begin{eqnarray}\nJ(\\mathbf{w}) = -\\log P(\\mathbf{y}|\\mathbf{X}; \\mathbf{w}) \\newline\n= -\\sum_{i=1}^N(y_i \\log {z}_i + (1-y_i) \\log (1 - {z}_i))\n\\end{eqnarray}\n\\]\nvới chú ý rằng \\(z_i\\) là một hàm số của \\(\\mathbf{w}\\). Bạn đọc tạm nhớ biểu thức vế phải có tên gọi làcross entropy, thường được sử dụng để đokhoảng cáchgiữa hai phân phối (distributions). Trong bài toán đang xét, một phân phối là dữ liệu được cho, với xác suất chỉ là 0 hoặc 1; phân phối còn lại được tính theo mô hình logistic regression.Khoảng cáchgiữa hai phân phối nhỏ đồng nghĩa với việc (có vẻ hiển nhiên là) hai phân phối đó rất gần nhau. Tính chất cụ thể của hàm số này sẽ được đề cập trong một bài khác mà tầm quan trọng của khoảng cách giữa hai phân phối là lớn hơn.Chú ý:Trong machine learning, logarit thập phân ít được dùng, vì vậy \\(\\log\\) thường được dùng để ký hiệu logarit tự nhiên.Tối ưu hàm mất mátChúng ta lại sử dụng phương phápStochastic Gradient Descent(SGD) ở đây (Bạn đọc được khuyến khích đọc SGD trước khi đọc phần này) . Hàm mất mát với chỉ một điểm dữ liệu \\((\\mathbf{x}_i, y_i)\\) là:\n\\[\nJ(\\mathbf{w}; \\mathbf{x}_i, y_i) = -(y_i \\log {z}_i + (1-y_i) \\log (1 - {z}_i))\n\\]Với đạo hàm:\n\\[\n\\begin{eqnarray}\n\\frac{\\partial J(\\mathbf{w}; \\mathbf{x}_i, y_i)}{\\partial \\mathbf{w}} &=& -(\\frac{y_i}{z_i} - \\frac{1- y_i}{1 - z_i} ) \\frac{\\partial z_i}{\\partial \\mathbf{w}} \\newline\n&=& \\frac{z_i - y_i}{z_i(1 - z_i)} \\frac{\\partial z_i}{\\partial \\mathbf{w}} ~~~~~~ (3)\n\\end{eqnarray}\n\\]Để cho biểu thức này trở nêngọnvàđẹphơn, chúng ta sẽ tìm hàm \\(z = f(\\mathbf{w}^T\\mathbf{x})\\) sao cho mẫu số bị triệt tiêu. Nếu đặt \\(s = \\mathbf{w}^T\\mathbf{x}\\), chúng ta sẽ có:\n\\[\n\\frac{\\partial z_i}{\\partial \\mathbf{w}} = \\frac{\\partial z_i}{\\partial s} \\frac{\\partial s}{\\partial \\mathbf{w}} = \\frac{\\partial z_i}{\\partial s} \\mathbf{x}\n\\]\nMột cách trực quan nhất, ta sẽ tìm hàm số \\(z = f(s)\\) sao cho:\n\\[\n\\frac{\\partial z}{\\partial s} = z(1 - z) ~~ (4)\n\\]\nđể triệt tiêu mẫu số trong biểu thức \\((3)\\). Chúng ta cùng khởi động một chút với phương trình vi phân đơn giản này. Phương trình \\((4)\\) tương đương với:\n\\[\n\\begin{eqnarray}\n&\\frac{\\partial z}{z(1-z)} &=& \\partial s \\newline\n\\Leftrightarrow & (\\frac{1}{z} + \\frac{1}{1 - z})\\partial z &=&\\partial s \\newline\n\\Leftrightarrow & \\log z - \\log(1 - z) &=& s \\newline\n\\Leftrightarrow & \\log \\frac{z}{1 - z} &=& s \\newline\n\\Leftrightarrow & \\frac{z}{1 - z} &=& e^s \\newline\n\\Leftrightarrow & z &=& e^s (1 - z) \\newline\n\\Leftrightarrow & z = \\frac{e^s}{1 +e^s} &=&\\frac{1}{1 + e^{-s}} = \\sigma(s)\n\\end{eqnarray}\n\\]\nĐến đây, tôi hy vọng các bạn đã hiểu hàm sốsigmoidđược tạo ra như thế nào.Chú ý: Trong việc giải phương trình vi phân ở trên, tôi đã bỏ qua hằng số khi lấy nguyên hàm hai vế. Tuy vậy, việc này không ảnh hưởng nhiều tới kết quả.Công thức cập nhật cho logistic sigmoid regressionTới đây, bạn đọc có thể kiểm tra rằng:\n\\[\n\\frac{\\partial J(\\mathbf{w}; \\mathbf{x}_i, y_i)}{\\partial \\mathbf{w}} = (z_i - y_i)\\mathbf{x}_i\n\\]\nQúa đẹp!Và công thức cập nhật (theo thuật toánSGD) cho logistic regression là: \n\\[\n\\mathbf{w} = \\mathbf{w} + \\eta(y_i - z_i)\\mathbf{x}_i\n\\]\nKhá đơn giản! Và, như thường lệ, chúng ta sẽ có vài ví dụ với Python.3. Ví dụ với PythonVí dụ với dữ liệu 1 chiềuQuay trở lại với ví dụ nêu ở phần Giới thiệu. Trước tiên ta cần khai báo vài thư viện và dữ liệu:# To support both python 2 and python 3from__future__importdivision,print_function,unicode_literalsimportnumpyasnpimportmatplotlib.pyplotaspltnp.random.seed(2)X=np.array([[0.50,0.75,1.00,1.25,1.50,1.75,1.75,2.00,2.25,2.50,2.75,3.00,3.25,3.50,4.00,4.25,4.50,4.75,5.00,5.50]])y=np.array([0,0,0,0,0,0,1,0,1,0,1,0,1,0,1,1,1,1,1,1])# extended dataX=np.concatenate((np.ones((1,X.shape[1])),X),axis=0)Các hàm cần thiết cho logistic sigmoid regressiondefsigmoid(s):return1/(1+np.exp(-s))deflogistic_sigmoid_regression(X,y,w_init,eta,tol=1e-4,max_count=10000):w=[w_init]it=0N=X.shape[1]d=X.shape[0]count=0check_w_after=20whilecount<max_count:# mix datamix_id=np.random.permutation(N)foriinmix_id:xi=X[:,i].reshape(d,1)yi=y[i]zi=sigmoid(np.dot(w[-1].T,xi))w_new=w[-1]+eta*(yi-zi)*xicount+=1# stopping criteriaifcount%check_w_after==0:ifnp.linalg.norm(w_new-w[-check_w_after])<tol:returnww.append(w_new)returnweta=.05d=X.shape[0]w_init=np.random.randn(d,1)w=logistic_sigmoid_regression(X,y,w_init,eta)print(w[-1])[[-4.092695  ]\n [ 1.55277242]]Với kết quả tìm được, đầu ra \\(y\\) có thể được dự đoán theo công thức:y = sigmoid(-4.1 + 1.55*x). Với dữ liệu trong tập training, kết quả là:print(sigmoid(np.dot(w[-1].T,X)))[[ 0.03281144  0.04694533  0.06674738  0.09407764  0.13102736  0.17961209\n   0.17961209  0.24121129  0.31580406  0.40126557  0.49318368  0.58556493\n   0.67229611  0.74866712  0.86263755  0.90117058  0.92977426  0.95055357\n   0.96541314  0.98329067]]Biểu diễn kết quả này trên đồ thị ta có:X0=X[1,np.where(y==0)][0]y0=y[np.where(y==0)]X1=X[1,np.where(y==1)][0]y1=y[np.where(y==1)]plt.plot(X0,y0,'ro',markersize=8)plt.plot(X1,y1,'bs',markersize=8)xx=np.linspace(0,6,1000)w0=w[-1][0][0]w1=w[-1][1][0]threshold=-w0/w1yy=sigmoid(w0+w1*xx)plt.axis([-2,8,-1,2])plt.plot(xx,yy,'g-',linewidth=2)plt.plot(threshold,.5,'y^',markersize=8)plt.xlabel('studying hours')plt.ylabel('predicted probability of pass')plt.show()Hình 4: Dữ liệu và hàm sigmoid tìm được.Nếu như chỉ có hai output là ‘fail’ hoặc ‘pass’, điểm trên đồ thị của hàm sigmoid tương ứng với xác suất 0.5 được chọn làmhard threshold(ngưỡng cứng). Việc này có thể chứng minh khá dễ dàng (tôi sẽ bàn ở phần dưới).Ví dụ với dữ liệu 2 chiềuChúng ta xét thêm một ví dụ nhỏ nữa trong không gian hai chiều. Giả sử chúng ta có hai class xanh-đỏ với dữ liệu được phân bố như hình dưới.Hình 5: Hai class với dữ liệu hai chiều.Với dữ liệu đầu vào nằm trong không gian hai chiều, hàm sigmoid có dạng như thác nước dưới đây:Hình 6: Hàm sigmoid với dữ liệu có chiều là 2. (Nguồn:Biased and non biased neurons)Kết quả tìm được khi áp dụng mô hình logistic regression được minh họa như hình dưới với màu nền khác nhau thể hiện xác suất điểm đó thuộc class đỏ. Đỏ hơn tức gần 1 hơn, xanh hơn tức gần 0 hơn.Hình 7: Logistic Regression với dữ liệu hai chiều.Nếu phải lựa chọn mộtngưỡng cứng(chứ không chấp nhận xác suất) để phân chia hai class, chúng ta quan sát thấy đường thẳng nằm nằm trong khu vực xanh lục là một lựa chọn hợp lý. Tôi sẽ chứng minh ở phần dưới rằng, đường phân chia giữa hai class tìm được bởi logistic regression có dạng một đường phẳng, tức vẫn là linear.4. Một vài tính chất của Logistic RegressionLogistic Regression thực ra được sử dụng nhiều trong các bài toán Classification.Mặc dù có tên là Regression, tức một mô hình cho fitting, Logistic Regression lại được sử dụng nhiều trong các bài toán Classification. Sau khi tìm được mô hình, việc xác định class \\(y\\) cho một điểm dữ liệu \\(\\mathbf{x}\\) được xác định bằng việc so sánh hai biểu thức xác suất:\n\\[\nP(y = 1| \\mathbf{x}; \\mathbf{w}); ~~ P(y = 0| \\mathbf{x}; \\mathbf{w}) \n\\]\nNếu biểu thức thứ nhất lớn hơn thì ta kết luận điểm dữ liệu thuộc class 1, ngược lại thì nó thuộc class 0. Vì tổng hai biểu thức này luôn bằng 1 nên một cách gọn hơn, ta chỉ cần xác định xem \\(P(y = 1| \\mathbf{x}; \\mathbf{w})\\) lớn hơn 0.5 hay không. Nếu có, class 1. Nếu không, class 0.Boundary tạo bởi Logistic Regression có dạng tuyến tínhThật vậy, theo lập luận ở phần trên thì chúng ta cần kiểm tra:\\[\n\\begin{eqnarray}\nP(y = 1| \\mathbf{x}; \\mathbf{w}) &>& 0.5 \\newline\n\\Leftrightarrow \\frac{1}{1 + e^{-\\mathbf{w}^T\\mathbf{x}}} &>& 0.5 \\newline\n\\Leftrightarrow e^{-\\mathbf{w}^T\\mathbf{x}} &<& 1 \\newline\n\\Leftrightarrow \\mathbf{w}^T\\mathbf{x} &>& 0\n\\end{eqnarray}\n\\]Nói cách khác, boundary giữa hai class là đường có phương trình \\(\\mathbf{w}^T\\mathbf{x}\\). Đây chính là phương trình của một siêu mặt phẳng. Vậy Logistic Regression tạo ra boundary có dạng tuyến tính.5. Thảo luậnMột điểm cộng cho Logistic Regression so với PLA là nó không cần có giả thiết dữ liệu hai class là linearly separable. Tuy nhiên, boundary tìm được vẫn có dạng tuyến tính. Vậy nên mô hình này chỉ phù hợp với loại dữ liệu mà hai class là gần với linearly separable. Một kiểu dữ liệu mà Logistic Regression không làm việc được là dữ liệu mà\nmột class chứa các điểm nằm trong 1 vòng tròn, class kia chứa các điểm bên ngoài đường tròn đó. Kiểu dữ liệu này được gọi là phi tuyến (non-linear). Sau một vài bài nữa, tôi sẽ giới thiệu với các bạn các mô hình khác phù hợp hơn với loại dữ liệu này hơn.Một hạn chế nữa của Logistic Regression là nó yêu cầu các điểm dữ liệu được tạo ra một cáchđộc lậpvới nhau. Trên thực tế, các điểm dữ liệu có thể bịảnh hưởngbởi nhau. Ví dụ: có một nhóm ôn tập với nhau trong 4 giờ, cả nhóm đều thi đỗ (giả sử các bạn này học rất tập trung), nhưng có một sinh viên học một mình cũng trong 4 giờ thì xác suất thi đỗ thấp hơn. Mặc dù vậy, để cho đơn giản, khi xây dựng mô hình, người ta vẫn thường giả sử các điểm dữ liệu là độc lập với nhau.Khi biểu diễn theo Neural Networks, Linear Regression, PLA, và Logistic Regression có dạng như sau:Hình 8: Biểu diễn Linear Regression, PLA, và Logistic Regression theo Neural network.Nếu hàm mất mát của Logistic Regression được viết dưới dạng:\n\\[\nJ(\\mathbf{w}) = \\sum_{i=1}^N (y_i - z_i)^2\n\\]\nthì khó khăn gì sẽ xảy ra? Các bạn hãy coi đây như một bài tập nhỏ.Source code cho các ví dụ trong bài này có thểtìm thấy ở đây.6. Tài liệu tham khảo[1] Cox, David R. “The regression analysis of binary sequences.” Journal of the Royal Statistical Society. Series B (Methodological) (1958): 215-242.[2] Cramer, Jan Salomon. “The origins of logistic regression.” (2002).[3] Abu-Mostafa, Yaser S., Malik Magdon-Ismail, and Hsuan-Tien Lin. Learning from data. Vol. 4. New York, NY, USA:: AMLBook, 2012. (link to course)[4] Bishop, Christopher M. “Pattern recognition and Machine Learning.”, Springer  (2006). (book)[5] Duda, Richard O., Peter E. Hart, and David G. Stork. Pattern classification. John Wiley & Sons, 2012.[6] Andrer Ng. CS229 Lecture notes.Part II: Classification and logistic regression[7] Jerome H. Friedman, Robert Tibshirani, and Trevor Hastie.The Elements of Statistical Learning.",
        "summary": "Mô hình Logistic Regression là một mô hình tuyến tính được sử dụng rộng rãi trong các bài toán phân loại, cho phép dự đoán xác suất một điểm dữ liệu thuộc vào một lớp cụ thể. Hàm sigmoid, được sử dụng làm hàm kích hoạt trong Logistic Regression, được thiết kế để tạo ra một đầu ra bị chặn trong khoảng (0, 1), phù hợp với việc biểu diễn xác suất. Phương pháp tối ưu hóa hàm mất mát trong Logistic Regression thường sử dụng thuật toán Stochastic Gradient Descent (SGD), giúp tìm ra các tham số tối ưu cho mô hình dựa trên dữ liệu huấn luyện. \n",
        "status": true
    },
    "ML003": {
        "content": "Trong trang này:\nNaive Bayes Classifier\nCác phân phối thường dùng cho \\(p(x_i | c)\\)\n2.1 Gaussian Naive Bayes\n2.2. Multinomial Naive Bayes\n2.3. Bernoulli Naive Bayes\nVí dụ\n3.1. Bắc hay Nam\n3.2. Bắc hay Nam với sklearn\n3.3. Naive Bayes Classifier cho bài toán Spam Filtering\nTóm tắt\nTài liệu tham khảo\nBạn được khuyến khích đọc Bài 31: Maximum Likelihood và Maximum A Posteriori estimation trước khi đọc bài này\n1. Naive Bayes Classifier\nXét bài toán classification với \\(C\\) classes \\(1, 2, \\dots, C\\). Giả sử có một điểm dữ liệu \\(\\mathbf{x} \\in \\mathbb{R}^d\\). Hãy tính xác suất để điểm dữ liệu này rơi vào class \\(c\\). Nói cách khác, hãy tính:\n\\[\np(y = c |\\mathbf{x}) ~~~ (1)\n\\]\nhoặc viết gọn thành \\(p(c|\\mathbf{x})\\).\nTức tính xác suất để đầu ra là class \\(c\\) biết rằng đầu vào là vector \\(\\mathbf{x}\\).\nBiểu thức này, nếu tính được, sẽ giúp chúng ta xác định được xác suất để điểm dữ liệu rơi vào mỗi class. Từ đó có thể giúp xác định class của điểm dữ liệu đó bằng cách chọn ra class có xác suất cao nhất:\n\\[\nc = \\arg\\max_{c \\in \\{1, \\dots, C\\}} p(c | \\mathbf{x}) ~~~~ (2)\n\\]\nBiểu thức \\((2)\\) thường khó được tính trực tiếp. Thay vào đó, quy tắc Bayes thường được sử dụng:\n\\[\n\\begin{eqnarray}\nc & = & \\arg\\max_c p(c | \\mathbf{x}) & (3) \\\n& = & \\arg\\max_c \\frac{p(\\mathbf{x} | c) p(c)}{p(\\mathbf{x})} ~~~& 4)\\\n& = & \\arg\\max_c p(\\mathbf{x} | c) p(c) & (5)\\\n\\end{eqnarray}\n\\]\nTừ \\((3)\\) sang \\((4)\\) là vì quy tắc Bayes. Từ \\((4)\\) sang \\((5)\\) là vì mẫu số \\(p(\\mathbf{x})\\) không phụ thuộc vào \\(c\\).\nTiếp tục xét biểu thức \\((5)\\), \\(p(c)\\) có thể được hiểu là xác suất để một điểm rơi vào class \\(c\\). Giá trị này có thể được tính bằng MLE, tức tỉ lệ số điểm dữ liệu trong tập training rơi vào class này chia cho tổng số lượng dữ liệu trong tập training; hoặc cũng có thể được đánh giá bằng MAP estimation. Trường hợp thứ nhất thường được sử dụng nhiều hơn.\nThành phần còn lại \\(p(\\mathbf{x} | c)\\), tức phân phối của các điểm dữ liệu trong class \\(c\\), thường rất khó tính toán vì \\(\\mathbf{x}\\) là một biến ngẫu nhiên nhiều chiều, cần rất rất nhiều dữ liệu training để có thể xây dựng được phân phối đó. Để giúp cho việc tính toán được đơn giản, người ta thường giả sử một cách đơn giản nhất rằng các thành phần của biến ngẫu nhiên \\(\\mathbf{x}\\) là độc lập với nhau, nếu biết \\(c\\) (given \\(c\\)). Tức là:\n\\[\np(\\mathbf{x} | c) = p(x_1, x_2, \\dots, x_d | c) =  \\prod_{i = 1}^d p(x_i | c) ~~~~~ (6)\n\\]\nGiả thiết các chiều của dữ liệu độc lập với nhau, nếu biết \\(c\\), là quá chặt và ít khi tìm được dữ liệu mà các thành phần hoàn toàn độc lập với nhau. Tuy nhiên, giả thiết ngây ngô này lại mang lại những kết quả tốt bất ngờ. Giả thiết về sự độc lập của các chiều dữ liệu này được gọi là Naive Bayes (xin không dịch). Cách xác định class của dữ liệu dựa trên giả thiết này có tên là Naive Bayes Classifier (NBC).\nNBC, nhờ vào tính đơn giản một cách ngây thơ, có tốc độ training và test rất nhanh. Việc này giúp nó mang lại hiệu quả cao trong các bài toán large-scale.\nỞ bước training, các phân phối \\(p(c)\\) và \\(p(x_i | c), i = 1, \\dots, d\\) sẽ được xác định dựa vào training data. Việc xác định các giá trị này có thể dựa vào Maximum Likelihood Estimation hoặc Maximum A Posteriori.\nỞ bước test, với một điểm dữ liệu mới \\(\\mathbf{x}\\), class của nó sẽ được xác đinh bởi:\n\\[\nc = \\arg\\max_{c \\in \\{1, \\dots, C\\}} p(c) \\prod_{i=1}^d p(x_i | c) ~~~~~ (7)\n\\]\nKhi \\(d\\) lớn và các xác suất nhỏ, biểu thức ở vế phải của \\((7)\\) sẽ là một số rất nhỏ, khi tính toán có thể gặp sai số. Để giải quyết việc này, \\((7)\\) thường được viết lại dưới dạng tương đương bằng cách lấy \\(\\log\\) của vế phải:\n\\[\nc = \\arg\\max_{c \\in \\{1, \\dots, C\\}} = \\log(p(c)) + \\sum_{i=1}^d \\log(p(x_i | c)) ~~~~ (7.1)\n\\]\nViệc này không ảnh hưởng tới kết quả vì \\(\\log\\) là một hàm đồng biến trên tập các số dương.\nMặc dù giả thiết mà Naive Bayes Classifiers sử dụng là quá phi thực tế, chúng vẫn hoạt động khá hiệu quả trong nhiều bài toán thực tế, đặc biệt là trong các bài toán phân loại văn bản, ví dụ như lọc tin nhắn rác hay lọc email spam. Trong phần sau của bài viết, chúng ta cùng xây dựng một bộ lọc email spam tiếng Anh đơn giản.\nCả việc training và test của NBC là cực kỳ nhanh khi so với các phương pháp classification phức tạp khác. Việc giả sử các thành phần trong dữ liệu là độc lập với nhau, nếu biết class, khiến cho việc tính toán mỗi phân phối \\(p(\\mathbf{x}_i|c)\\) trở nên cực kỳ nhanh.\nMỗi giá trị \\(p(c), c = 1, 2, \\dots, C\\) có thể được xác định như là tần suất xuất hiện của class \\(c\\) trong training data.\nViệc tính toán \\(p(\\mathbf{x_i} | c) \\) phụ thuộc vào loại dữ liệu. Có ba loại được sử dụng phổ biến là: Gaussian Naive Bayes, Multinomial Naive Bayes, và Bernoulli Naive .\n2. Các phân phối thường dùng cho \\(p(x_i | c)\\)\nMục này chủ yếu được dịch từ tài liệu của thư viện sklearn.\n2.1 Gaussian Naive Bayes\nMô hình này được sử dụng chủ yếu trong loại dữ liệu mà các thành phần là các biến liên tục.\nVới mỗi chiều dữ liệu \\(i\\) và một class \\(c\\), \\(x_i\\) tuân theo một phân phối chuẩn có kỳ vọng \\(\\mu_{ci}\\) và phương sai \\(\\sigma_{ci}^2\\):\n\\[\np(x_i|c) = p(x_i | \\mu_{ci}, \\sigma_{ci}^2) =  \\frac{1}{\\sqrt{2\\pi \\sigma_{ci}^2}} \\exp\\left(- \\frac{(x_i - \\mu_{ci})^2}{2 \\sigma_{ci}^2}\\right) ~~~~ (8)\n\\]\nTrong đó, bộ tham số \\(\\theta = \\{\\mu_{ci}, \\sigma_{ci}^2\\}\\) được xác định bằng Maximum Likelihood:\n\\[\n(\\mu_{ci}, \\sigma_{ci}^2) = \\arg\\max_{\\mu_{ci}, \\sigma_{ci}^2} \\prod_{n = 1}^N p(x_i^{(n)} | \\mu_{ci}, \\sigma_{ci}^2) ~~~~ (9)\n\\]\nĐây là cách tính của thư viện sklearn. Chúng ta cũng có thể đánh giá các tham số bằng MAP nếu biết trước priors của \\(\\mu_{ci}\\) và \\(\\sigma^2_{ci}\\)\n2.2. Multinomial Naive Bayes\nMô hình này chủ yếu được sử dụng trong phân loại văn bản mà feature vectors được tính bằng Bags of Words. Lúc này, mỗi văn bản được biểu diễn bởi một vector có độ dài \\(d\\) chính là số từ trong từ điển. Giá trị của thành phần thứ \\(i\\) trong mỗi vector chính là số lần từ thứ \\(i\\) xuất hiện trong văn bản đó.\nKhi đó, \\(p(x_i |c) \\) tỉ lệ với tần suất từ thứ \\(i\\) (hay feature thứ \\(i\\) cho trường hợp tổng quát) xuất hiện trong các văn bản của class \\(c\\). Giá trị này có thể được tính bằng cách:\n\\[\n\\lambda_{ci} = p(x_i | c) = \\frac{N_{ci}}{N_c} ~~~~ (10)\n\\]\nTrong đó:\n\\(N_{ci}\\) là tổng số lần từ thứ \\(i\\) xuất hiện trong các văn bản của class \\(c\\), nó được tính là tổng của tất cả các thành phần thứ \\(i\\) của các feature vectors ứng với class \\(c\\).\n\\(N_c\\) là tổng số từ (kể cả lặp) xuất hiện trong class \\(c\\). Nói cách khác, nó bằng tổng độ dài của toàn bộ các văn bản thuộc vào class \\(c\\). Có thể suy ra rằng \\(N_c = \\sum_{i = 1}^d N_{ci}\\), từ đó \\(\\sum_{i=1}^d \\lambda_{ci} = 1\\).\nCách tính này có một hạn chế là nếu có một từ mới chưa bao giờ xuất hiện trong class \\(c\\) thì biểu thức \\((10)\\) sẽ bằng 0, điều này dẫn đến vế phải của \\((7)\\) bằng 0 bất kể các giá trị còn lại có lớn thế nào. Việc này sẽ dẫn đến kết quả không chính xác (xem thêm ví dụ ở mục sau).\nĐể giải quyết việc này, một kỹ thuật được gọi là Laplace smoothing được áp dụng:\n\\[\n\\hat{\\lambda}_{ci} = \\frac{N_{ci} + \\alpha}{N_{c} + d\\alpha} ~~~~~~ (11)\n\\]\nVới \\(\\alpha\\) là một số dương, thường bằng 1, để tránh trường hợp tử số bằng 0. Mẫu số được cộng với \\(d\\alpha\\) để đảm bảo tổng xác suất \\(\\sum_{i=1}^d \\hat{\\lambda}_{ci} = 1\\).\nNhư vậy, mỗi class \\(c\\) sẽ được mô tả bởi bộ các số dương có tổng bằng 1: \\(\\hat{\\lambda}_c = \\{\\hat{\\lambda}_{c1}, \\dots, \\hat{\\lambda}_{cd}\\}\\).\n2.3. Bernoulli Naive Bayes\nMô hình này được áp dụng cho các loại dữ liệu mà mỗi thành phần là một giá trị binary - bẳng 0 hoặc 1. Ví dụ: cũng với loại văn bản nhưng thay vì đếm tổng số lần xuất hiện của 1 từ trong văn bản, ta chỉ cần quan tâm từ đó có xuất hiện hay không.\nKhi đó, \\(p(x_i | c) \\) được tính bằng:\n\\[\np(x_i | c) = p(i | c)^{x_i} (1 - p(i | c) ^{1 - x_i}\n\\]\nvới \\(p(i | c)\\) có thể được hiểu là xác suất từ thứ \\(i\\) xuất hiện trong các văn bản của class \\(c\\).\n3. Ví dụ\n3.1. Bắc hay Nam\nGiả sử trong tập training có các văn bản \\(\\text{d1, d2, d3, d4}\\) như trong bảng dưới đây. Mỗi văn bản này thuộc vào 1 trong 2 classes: \\(\\text{B}\\) (Bắc) hoặc \\(\\text{N}\\) (Nam). Hãy xác định class của văn bản \\(\\text{d5}\\).\nDocument\nContent\nClass\nTraining\n\\(\\text{d1}\\)\n\\(\\text{hanoi pho chaolong hanoi}\\)\n\\(\\text{B}\\)\n\\(\\text{d2}\\)\n\\(\\text{hanoi buncha pho omai}\\)\n\\(\\text{B}\\)\n\\(\\text{d3}\\)\n\\(\\text{pho banhgio omai}\\)\n\\(\\text{B}\\)\n\\(\\text{d4}\\)\n\\(\\text{saigon hutiu banhbo pho}\\)\n\\(\\text{N}\\)\nTest\n\\(\\text{d5}\\)\n\\(\\text{hanoi hanoi buncha hutiu}\\)\n?\nTa có thể dự đoán rằng \\(\\text{d5}\\) thuộc class Bắc.\nBài toán này có thể được giải quyết bởi hai mô hình: Multinomial Naive Bayes và Bernoulli Naive Bayes. Tôi sẽ làm ví dụ minh hoạ với mô hình thứ nhất và thực hiện code cho cả hai mô hình. Việc mô hình nào tốt hơn phụ thuộc vào mỗi bài toán. Chúng ta có thể thử cả hai để chọn ra mô hình tốt hơn.\nNhận thấy rằng ở đây có 2 class \\(\\text{B}\\) và \\(\\text{N}\\), ta cần đi tìm \\(p(\\text{B})\\) và \\(p(\\text{N})\\). à dựa trên tần số xuất hiện của mỗi class trong tập training. Ta sẽ có:\n\\[\np(\\text{B}) = \\frac{3}{4}, ~~~~~ p(\\text{N}) = \\frac{1}{4} ~~~~~~ (8)\n\\]\nTập hợp toàn bộ các từ trong văn bản, hay còn gọi là từ điển, là: \\(V = \\{\\text{hanoi, pho, chaolong, buncha, omai, banhgio, saigon, hutiu, banhbo}\\}\\). Tổng cộng số phần tử trong từ điển là \\(|V| = 9\\).\nHình dưới đây minh hoạ quá trình Training và Test cho bài toán này khi sử dụng Multinomial Naive Bayes, trong đó có sử dụng Laplace smoothing với \\(\\alpha = 1\\).\nHình 1: Minh hoạ Multinomial Naive Bayes.\nChú ý, hai giá trị tìm được \\(1.5\\times 10^{-4}\\) và \\(1.75\\times 10^{-5}\\) không phải là hai xác suất cần tìm mà chỉ là hai đại lượng tỉ lệ thuận với hai xác suất đó. Để tính cụ thể, ta có thể làm như sau:\n\\[\np(\\text{B} | \\text{d5}) = \\frac{1.5\\times 10^{-4}}{1.5\\times 10^{-4} + 1.75\\times 10^{-5}} \\approx 0.8955, ~~~~ p(\\text{N} | \\text{d5}) = 1 - p(\\text{B} | \\text{d5}) \\approx 0.1045\n\\]\nBạn đọc có thể tự tính với ví dụ khác: \\(\\text{d6 = pho hutiu banhbo}\\). Nếu bạn và tôi tính ra kết quả giống nhau, chúng ta sẽ thu được:\n\\[\np(\\text{B} | \\text{d6}) \\approx 0.29, ~~~~ p(\\text{N} | \\text{d6}) \\approx 0.71\n\\]\nvà suy ra \\(\\text{d6}\\) thuộc vào class Nam.\n3.2. Bắc hay Nam với sklearn\nĐể kiểm tra lại các phép tính toán phía trên, chúng ta cùng giải quyết bài toán này với sklearn.\nfrom __future__ import print_function\nfrom sklearn.naive_bayes import MultinomialNB\nimport numpy as np\n# train data\nd1 = [2, 1, 1, 0, 0, 0, 0, 0, 0]\nd2 = [1, 1, 0, 1, 1, 0, 0, 0, 0]\nd3 = [0, 1, 0, 0, 1, 1, 0, 0, 0]\nd4 = [0, 1, 0, 0, 0, 0, 1, 1, 1]\ntrain_data = np.array([d1, d2, d3, d4])\nlabel = np.array(['B', 'B', 'B', 'N'])\n# test data\nd5 = np.array([[2, 0, 0, 1, 0, 0, 0, 1, 0]])\nd6 = np.array([[0, 1, 0, 0, 0, 0, 0, 1, 1]])\n## call MultinomialNB\nclf = MultinomialNB()\n# training\nclf.fit(train_data, label)\n# test\nprint('Predicting class of d5:', str(clf.predict(d5)[0]))\nprint('Probability of d6 in each class:', clf.predict_proba(d6))\nKết quả:\nPredicting class of d5: B\nProbability of d6 in each class: [[ 0.29175335  0.70824665]]\nNếu sử dụng mô hình Bernoulli Naive Bayes, chúng ta cần thay đổi một chút về feature vectors. Lúc này, các giá trị khác không sẽ đều được đưa về 1 vì ta chỉ quan tâm đến việc từ đó có xuất hiện trong văn bản không.\nfrom __future__ import print_function\nfrom sklearn.naive_bayes import BernoulliNB\nimport numpy as np\n# train data\nd1 = [1, 1, 1, 0, 0, 0, 0, 0, 0]\nd2 = [1, 1, 0, 1, 1, 0, 0, 0, 0]\nd3 = [0, 1, 0, 0, 1, 1, 0, 0, 0]\nd4 = [0, 1, 0, 0, 0, 0, 1, 1, 1]\ntrain_data = np.array([d1, d2, d3, d4])\nlabel = np.array(['B', 'B', 'B', 'N']) # 0 - B, 1 - N\n# test data\nd5 = np.array([[1, 0, 0, 1, 0, 0, 0, 1, 0]])\nd6 = np.array([[0, 1, 0, 0, 0, 0, 0, 1, 1]])\n## call MultinomialNB\nclf = BernoulliNB()\n# training\nclf.fit(train_data, label)\n# test\nprint('Predicting class of d5:', str(clf.predict(d5)[0]))\nprint('Probability of d6 in each class:', clf.predict_proba(d6))\nKết quả:\nPredicting class of d5: B\nProbability of d6 in each class: [[ 0.16948581  0.83051419]]\nTa thấy rằng, với bài toán nhỏ này, cả hai mô hình đều cho kết quả giống nhau (xác suất tìm được khác nhau nhưng không ảnh hưởng tới quyết định cuối cùng).\n3.3. Naive Bayes Classifier cho bài toán Spam Filtering\nDữ liệu trong ví dụ này được lấy trong Exercise 6: Naive Bayes - Machine Learning - Andrew Ng.\nTrong ví dụ này, dữ liệu đã được xử lý, và là một tập con của cơ sở dữ liệu Ling-Spam Dataset.\nMô tả dữ liệu:\nTập dữ liệu này bao gồm tổng cộng 960 emails tiếng Anh, được tách thành tập training và test theo tỉ lệ 700:260, 50% trong mỗi tập là các spam emails.\nDữ liệu trong cơ sở dữ liệu này đã được xử lý khá đẹp. Các quy tắc xử lý như sau:\nLoại bỏ stop words: Những từ xuất hiện thường xuyên như ‘and’, ‘the’, ‘of’, … được loại bỏ.\nLemmatization: Những từ có cùng ‘gốc’ được đưa về cùng loại. Ví dụ, ‘include’, ‘includes’, ‘included’ đều được đưa chung về ‘include’. Tất cả các từ cũng đã được đưa về dạng ký tự thường (không phải HOA).\nLoại bỏ non-words: Số, dấu câu, ký tự ‘tabs’, ký tự ‘xuống dòng’ đã được loại bỏ.\nDưới đây là một ví dụ của 1 email không phải spam, trước khi được xử lý:\nSubject: Re: 5.1344 Native speaker intuitions\nThe discussion on native speaker intuitions has been extremely interesting,\nbut I worry that my brief intervention may have muddied the waters. I take\nit that there are a number of separable issues. The first is the extent to\nwhich a native speaker is likely to judge a lexical string as grammatical\nor ungrammatical per se. The second is concerned with the relationships\nbetween syntax and interpretation (although even here the distinction may\nnot be entirely clear cut).\nvà sau khi được xử lý:\nre native speaker intuition discussion native speaker intuition extremely\ninterest worry brief intervention muddy waters number separable issue first\nextent native speaker likely judge lexical string grammatical ungrammatical\nper se second concern relationship between syntax interpretation although\neven here distinction entirely clear cut\nVà đây là một ví dụ về spam email sau khi được xử lý:\nfinancial freedom follow financial freedom work ethic extraordinary desire\nearn least per month work home special skills experience required train\npersonal support need ensure success legitimate homebased income\nopportunity put back control finance life ve try opportunity past\nfail live promise\nChúng ta thấy rằng trong đoạn này có các từ như: financial, extraordinary, earn, opportunity, … là những từ thường thấy trong các email spam.\nTrong ví dụ này, chúng ta sẽ sử dụng Multinomial Naive Bayes.\nĐể cho bài toán được đơn giản hơn, tôi tiếp tục sử dụng dữ liệu đã được xử lý, có thể được download ở đây: ex6DataPrepared.zip. Trong folder sau khi giải nén, chúng ta sẽ thấy các files:\ntest-features.txt\ntest-labels.txt\ntrain-features-50.txt\ntrain-features-100.txt\ntrain-features-400.txt\ntrain-features.txt\ntrain-labels-50.txt\ntrain-labels-100.txt\ntrain-labels-400.txt\ntrain-labels.txt\ntương ứng với các file chứa dữ liệu của tập training và tập test. File train-features-50.txt chứa dữ liệu của tập training thu gọn với chỉ có tổng cộng 50 training emails.\nMỗi file *labels*.txt chứa nhiều dòng, mỗi dòng là một ký tự 0 hoặc 1 thể hiện email là non-spam hoặc spam.\nMỗi file *features*.txt chứa nhiều dòng, mỗi dòng có 3 số, ví dụ:\n1 564 1\n1 19 2\ntrong đó số đầu tiên là chỉ số của email, bắt đầu từ 1; số thứ hai là thứ tự của từ trong từ điển (tổng cộng 2500 từ); số thứ ba là số lượng của từ đó trong email đang xét. Dòng đầu tiên nói rằng trong email thứ nhất, từ thứ 564 trong từ điển xuất hiện 1 lần. Cách lưu dữ liệu như thế này giúp tiết kiệm bộ nhớ vì 1 email thường không chứa hết tất cả các từ trong từ điển mà chỉ chứa một lượng nhỏ, ta chỉ cần lưu các giá trị khác không.\nNếu ta biểu diễn feature vector của mỗi email là một vector hàng có độ dài bằng độ dài từ điển (2500) thì dòng thứ nhất nói rằng thành phần thứ 564 của vector này bằng 1. Tương tự, thành phần thứ 19 của vector này bằng 1. Nếu không xuất hiện, các thành phần khác được mặc định bằng 0.\nDựa trên các thông tin này, chúng ta có thể tiến hành lập trình với thư viện sklearn.\nKhai báo thư viện và đường dẫn tới files:\n## packages\nfrom __future__ import division, print_function, unicode_literals\nimport numpy as np\nfrom scipy.sparse import coo_matrix # for sparse matrix\nfrom sklearn.naive_bayes import MultinomialNB, BernoulliNB\nfrom sklearn.metrics import accuracy_score # for evaluating results\n# data path and file name\npath = 'ex6DataPrepared/'\ntrain_data_fn = 'train-features.txt'\ntest_data_fn = 'test-features.txt'\ntrain_label_fn = 'train-labels.txt'\ntest_label_fn = 'test-labels.txt'\nHàm số đọc dữ liệu từ file data_fn với labels tương ứng label_fn. Chú ý rằng số lượng từ trong từ điển là 2500.\nDữ liệu sẽ được lưu trong một ma trận mà mỗi hàng thể hiện một email. Ma trận này là một ma trận sparse nên chúng ta sẽ sử dụng hàm scipy.sparse.coo_matrix.\nnwords = 2500\ndef read_data(data_fn, label_fn):\n## read label_fn\nwith open(path + label_fn) as f:\ncontent = f.readlines()\nlabel = [int(x.strip()) for x in content]\n## read data_fn\nwith open(path + data_fn) as f:\ncontent = f.readlines()\n# remove '\\n' at the end of each line\ncontent = [x.strip() for x in content]\ndat = np.zeros((len(content), 3), dtype = int)\nfor i, line in enumerate(content):\na = line.split(' ')\ndat[i, :] = np.array([int(a[0]), int(a[1]), int(a[2])])\n# remember to -1 at coordinate since we're in Python\n# check this: https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.coo_matrix.html\n# for more information about coo_matrix function\ndata = coo_matrix((dat[:, 2], (dat[:, 0] - 1, dat[:, 1] - 1)),\\\nshape=(len(label), nwords))\nreturn (data, label)\nĐọc training data và test data, sử dụng class MultinomialNB trong sklearn để xây dựng mô hình và dự đoán đầu ra cho test data.\n(train_data, train_label)  = read_data(train_data_fn, train_label_fn)\n(test_data, test_label)  = read_data(test_data_fn, test_label_fn)\nclf = MultinomialNB()\nclf.fit(train_data, train_label)\ny_pred = clf.predict(test_data)\nprint('Training size = %d, accuracy = %.2f%%' % \\\n(train_data.shape[0],accuracy_score(test_label, y_pred)*100))\nTraining size = 700, accuracy = 98.08%\nVậy là có tới 98.08% các email được phân loại đúng. Chúng ta tiếp tục thử với các bộ dữ liệu training nhỏ hơn:\ntrain_data_fn = 'train-features-100.txt'\ntrain_label_fn = 'train-labels-100.txt'\ntest_data_fn = 'test-features.txt'\ntest_label_fn = 'test-labels.txt'\n(train_data, train_label)  = read_data(train_data_fn, train_label_fn)\n(test_data, test_label)  = read_data(test_data_fn, test_label_fn)\nclf = MultinomialNB()\nclf.fit(train_data, train_label)\ny_pred = clf.predict(test_data)\nprint('Training size = %d, accuracy = %.2f%%' % \\\n(train_data.shape[0],accuracy_score(test_label, y_pred)*100))\nTraining size = 100, accuracy = 97.69%\ntrain_data_fn = 'train-features-50.txt'\ntrain_label_fn = 'train-labels-50.txt'\ntest_data_fn = 'test-features.txt'\ntest_label_fn = 'test-labels.txt'\n(train_data, train_label)  = read_data(train_data_fn, train_label_fn)\n(test_data, test_label)  = read_data(test_data_fn, test_label_fn)\nclf = MultinomialNB()\nclf.fit(train_data, train_label)\ny_pred = clf.predict(test_data)\nprint('Training size = %d, accuracy = %.2f%%' % \\\n(train_data.shape[0],accuracy_score(test_label, y_pred)*100))\nTraining size = 50, accuracy = 97.31%\nTa thấy rằng thậm chí khi tập training là rất nhỏ, 50 emails tổng cộng, kết quả đạt được đã rất ấn tượng.\nNếu bạn muốn tiếp tục thử mô hình BernoulliNB:\nclf = BernoulliNB(binarize = .5)\nclf.fit(train_data, train_label)\ny_pred = clf.predict(test_data)\nprint('Training size = %d, accuracy = %.2f%%' % \\\n(train_data.shape[0],accuracy_score(test_label, y_pred)*100))\nTraining size = 50, accuracy = 69.62%\nTa thấy rằng trong bài toán này, MultinomialNB hoạt động hiệu quả hơn.\n4. Tóm tắt\nNaive Bayes Classifiers (NBC) thường được sử dụng trong các bài toán Text Classification.\nNBC có thời gian training và test rất nhanh. Điều này có được là do giả sử về tính độc lập giữa các thành phần, nếu biết class.\nNếu giả sử về tính độc lập được thoả mãn (dựa vào bản chất của dữ liệu), NBC được cho là cho kết quả tốt hơn so với SVM và logistic regression khi có ít dữ liệu training.\nNBC có thể hoạt động với các feature vector mà một phần là liên tục (sử dụng Gaussian Naive Bayes), phần còn lại ở dạng rời rạc (sử dụng Multinomial hoặc Bernoulli).\nKhi sử dụng Multinomial Naive Bayes, Laplace smoothing thường được sử dụng để tránh trường hợp 1 thành phần trong test data chưa xuất hiện ở training data.\nSource code.\n5. Tài liệu tham khảo\n[1] Text Classification and Naive Bayes - Stanford\n[2] Exercise 6: Naive Bayes - Machine Learning - Andrew Ng\n[3] sklearn.naive_bayes\n[4] 6 Easy Steps to Learn Naive Bayes Algorithm (with code in Python)",
        "summary": "Naive Bayes Classifier (NBC) là một thuật toán phân loại đơn giản, hiệu quả, thường được sử dụng trong các bài toán phân loại văn bản, đặc biệt là lọc spam. NBC hoạt động dựa trên giả thiết ngây thơ về tính độc lập giữa các thành phần của dữ liệu, nếu biết class, giúp cho việc tính toán được đơn giản hóa và tốc độ training, test rất nhanh. Mặc dù giả thiết này có vẻ phi thực tế, NBC vẫn cho kết quả tốt trong nhiều trường hợp thực tế, đặc biệt khi lượng dữ liệu training ít. \n",
        "status": true
    },
    "ML004": {
        "content": "Trong trang này:\n1. Giới thiệu\n2. Phân tích toán học\nMột số ký hiệu toán học\nHàm mất mát và bài toán tối ưu\nThuật toán tối ưu hàm mất mát\nCố định \\(\\mathbf{M} \\), tìm \\(\\mathbf{Y}\\)\nCố định \\(\\mathbf{Y} \\), tìm \\(\\mathbf{M}\\)\nTóm tắt thuật toán\n3. Ví dụ trên Python\nGiới thiệu bài toán\nHiển thị dữ liệu trên đồ thị\nCác hàm số cần thiết cho K-means clustering\nKết quả tìm được bằng thư viện scikit-learn\n4. Thảo luận\nHạn chế\nChúng ta cần biết số lượng cluster cần clustering\nNghiệm cuối cùng phụ thuộc vào các centers được khởi tạo ban đầu\nCác cluster cần có só lượng điểm gần bằng nhau\nCác cluster cần có dạng hình tròn\nKhi một cluster nằm phía trong 1 cluster khác\n5. Tài liệu tham khảo\n1. Giới thiệu\nTrong bài trước, chúng ta đã làm quen với thuật toán Linear Regression - là thuật toán đơn giản nhất trong Supervised learning. Bài này tôi sẽ giới thiệu một trong những thuật toán cơ bản nhất trong Unsupervised learning - thuật toán K-means clustering (phân cụm K-means).\nTrong thuật toán K-means clustering, chúng ta không biết nhãn (label) của từng điểm dữ liệu. Mục đích là làm thể nào để phân dữ liệu thành các cụm (cluster) khác nhau sao cho dữ liệu trong cùng một cụm có tính chất giống nhau.\nVí dụ: Một công ty muốn tạo ra những chính sách ưu đãi cho những nhóm khách hàng khác nhau dựa trên sự tương tác giữa mỗi khách hàng với công ty đó (số năm là khách hàng; số tiền khách hàng đã chi trả cho công ty; độ tuổi; giới tính; thành phố; nghề nghiệp; …). Giả sử công ty đó có rất nhiều dữ liệu của rất nhiều khách hàng nhưng chưa có cách nào chia toàn bộ khách hàng đó thành một số nhóm/cụm khác nhau. Nếu một người biết Machine Learning được đặt câu hỏi này, phương pháp đầu tiên anh (chị) ta nghĩ đến sẽ là K-means Clustering. Vì nó là một trong những thuật toán đầu tiên mà anh ấy tìm được trong các cuốn sách, khóa học về Machine Learning. Và tôi cũng chắc rằng anh ấy đã đọc blog Machine Learning cơ bản. Sau khi đã phân ra được từng nhóm, nhân viên công ty đó có thể lựa chọn ra một vài khách hàng trong mỗi nhóm để quyết định xem mỗi nhóm tương ứng với nhóm khách hàng nào. Phần việc cuối cùng này cần sự can thiệp của con người, nhưng lượng công việc đã được rút gọn đi rất nhiều.\nÝ tưởng đơn giản nhất về cluster (cụm) là tập hợp các điểm ở gần nhau trong một không gian nào đó (không gian này có thể có rất nhiều chiều trong trường hợp thông tin về một điểm dữ liệu là rất lớn). Hình bên dưới là một ví dụ về 3 cụm dữ liệu (từ giờ tôi sẽ viết gọn là cluster).\nBài toán với 3 clusters.\nGiả sử mỗi cluster có một điểm đại diện (center) màu vàng. Và những điểm xung quanh mỗi center thuộc vào cùng nhóm với center đó. Một cách đơn giản nhất, xét một điểm bất kỳ, ta xét xem điểm đó gần với center nào nhất thì nó thuộc về cùng nhóm với center đó. Tới đây, chúng ta có một bài toán thú vị: Trên một vùng biển hình vuông lớn có ba đảo hình vuông, tam giác, và tròn màu vàng như hình trên. Một điểm trên biển được gọi là thuộc lãnh hải của một đảo nếu nó nằm gần đảo này hơn so với hai đảo kia . Hãy xác định ranh giới lãnh hải của các đảo.\nHình dưới đây là một hình minh họa cho việc phân chia lãnh hải nếu có 5 đảo khác nhau được biểu diễn bằng các hình tròn màu đen:\nPhân vùng lãnh hải của mỗi đảo. Các vùng khác nhau có màu sắc khác nhau.\nChúng ta thấy rằng đường phân định giữa các lãnh hải là các đường thẳng (chính xác hơn thì chúng là các đường trung trực của các cặp điểm gần nhau). Vì vậy, lãnh hải của một đảo sẽ là một hình đa giác.\nCách phân chia này trong toán học được gọi là Voronoi Diagram.\nTrong không gian ba chiều, lấy ví dụ là các hành tinh, thì (tạm gọi là) lãnh không của mỗi hành tinh sẽ là một đa diện. Trong không gian nhiều chiều hơn, chúng ta sẽ có những thứ (mà tôi gọi là) siêu đa diện (hyperpolygon).\nQuay lại với bài toán phân nhóm và cụ thể là thuật toán K-means clustering, chúng ta cần một chút phân tích toán học trước khi đi tới phần tóm tắt thuật toán ở phần dưới. Nếu bạn không muốn đọc quá nhiều về toán, bạn có thể bỏ qua phần này. (Tốt nhất là đừng bỏ qua, bạn sẽ tiếc đấy).\n2. Phân tích toán học\nMục đích cuối cùng của thuật toán phân nhóm này là: từ dữ liệu đầu vào và số lượng nhóm chúng ta muốn tìm, hãy chỉ ra center của mỗi nhóm và phân các điểm dữ liệu vào các nhóm tương ứng. Giả sử thêm rằng mỗi điểm dữ liệu chỉ thuộc vào đúng một nhóm.\nMột số ký hiệu toán học\nGiả sử có \\(N\\) điểm dữ liệu là \\( \\mathbf{X} = [\\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_N] \\in \\mathbb{R}^{d \\times N}\\) và \\(K < N\\) là số cluster chúng ta muốn phân chia. Chúng ta cần tìm các center \\( \\mathbf{m}_1, \\mathbf{m}_2, \\dots, \\mathbf{m}_K \\in \\mathbb{R}^{d \\times 1} \\) và label của mỗi điểm dữ liệu.\nLưu ý về ký hiệu toán học: trong các bài viết của tôi, các số vô hướng được biểu diễn bởi các chữ cái viết ở dạng không in đậm, có thể viết hoa, ví dụ \\(x_1, N, y, k\\). Các vector được biểu diễn bằng các chữ cái thường in đậm, ví dụ \\(\\mathbf{m}, \\mathbf{x}_1 \\). Các ma trận được biểu diễn bởi các chữ viết hoa in đậm, ví dụ \\(\\mathbf{X, M, Y} \\). Lưu ý này đã được nêu ở bài Linear Regression. Tôi xin được không nhắc lại trong các bài tiếp theo.\nVới mỗi điểm dữ liệu \\( \\mathbf{x}_i \\) đặt \\(\\mathbf{y}_i = [y_{i1}, y_{i2}, \\dots, y_{iK}]\\) là label vector của nó, trong đó nếu \\( \\mathbf{x}_i \\) được phân vào cluster \\(k\\) thì  \\(y_{ik} = 1\\) và \\(y_{ij} = 0, \\forall j \\neq k \\). Điều này có nghĩa là có đúng một phần tử của vector \\(\\mathbf{y}_i\\) là bằng 1 (tương ứng với cluster của \\(\\mathbf{x}_i \\)), các phần tử còn lại bằng 0. Ví dụ: nếu một điểm dữ liệu có label vector là \\([1,0,0,\\dots,0]\\) thì nó thuộc vào cluster 1, là \\([0,1,0,\\dots,0]\\) thì nó thuộc vào cluster 2, \\(\\dots\\). Cách mã hóa label của dữ liệu như thế này được gọi là biểu diễn one-hot. Chúng ta sẽ thấy cách biểu diễn one-hot này rất phổ biến trong Machine Learning ở các bài tiếp theo.\nRàng buộc của \\(\\mathbf{y}_i \\) có thể viết dưới dạng toán học như sau:\n\\[\ny_{ik} \\in \\{0, 1\\},~~~ \\sum_{k = 1}^K y_{ik} = 1 ~~~ (1)\n\\]\nHàm mất mát và bài toán tối ưu\nNếu ta coi center \\(\\mathbf{m}_k \\)  là center (hoặc representative) của mỗi cluster và ước lượng tất cả các điểm được phân vào cluster này bởi \\(\\mathbf{m}_k \\), thì một điểm dữ liệu \\(\\mathbf{x}_i \\) được phân vào cluster \\(k\\) sẽ bị sai số là \\( (\\mathbf{x}_i - \\mathbf{m}_k) \\). Chúng ta mong muốn sai số này có trị tuyệt đối nhỏ nhất nên (giống như trong bài Linear Regression) ta sẽ tìm cách để đại lượng sau đây đạt giá trị nhỏ nhất:\n\\[\n\\|\\mathbf{x}_i - \\mathbf{m}_k\\|_2^2\n\\]\nHơn nữa, vì \\(\\mathbf{x}_i \\) được phân vào cluster \\(k\\) nên \\(y_{ik} = 1, y_{ij} = 0, ~\\forall j \\neq k \\). Khi đó, biểu thức bên trên sẽ được viết lại là:\n\\[\ny_{ik}\\|\\mathbf{x}_i - \\mathbf{m}_k\\|_2^2 =  \\sum_{j=1}^K y_{ij}\\|\\mathbf{x}_i - \\mathbf{m}_j\\|_2^2\n\\]\n(Hy vọng chỗ này không quá khó hiểu)\nSai số cho toàn bộ dữ liệu sẽ là:\n\\[\n\\mathcal{L}(\\mathbf{Y}, \\mathbf{M}) = \\sum_{i=1}^N\\sum_{j=1}^K y_{ij} \\|\\mathbf{x}_i - \\mathbf{m}_j\\|_2^2\n\\]\nTrong đó \\( \\mathbf{Y} = [\\mathbf{y}_1; \\mathbf{y}_2; \\dots; \\mathbf{y}_N]\\), \\( \\mathbf{M} = [\\mathbf{m}_1, \\mathbf{m}_2, \\dots \\mathbf{m}_K] \\) lần lượt là các ma trận được tạo bởi label vector của mỗi điểm dữ liệu và center của mỗi cluster. Hàm số mất mát trong bài toán K-means clustering của chúng ta là hàm \\(\\mathcal{L}(\\mathbf{Y}, \\mathbf{M})\\) với ràng buộc như được nêu trong phương trình \\((1)\\).\nTóm lại, chúng ta cần tối ưu bài toán sau:\n\\[\n\\mathbf{Y}, \\mathbf{M} = \\arg\\min_{\\mathbf{Y}, \\mathbf{M}} \\sum_{i=1}^N\\sum_{j=1}^K y_{ij} \\|\\mathbf{x}_i - \\mathbf{m}_j\\|_2^2~~~~~(2)\n\\]\n\\[\n\\text{subject to:} ~~ y_{ij} \\in \\{0, 1\\}~~ \\forall i, j;~~~ \\sum_{j = 1}^K y_{ij} = 1~~\\forall i\n\\]\n(subject to nghĩa là thỏa mãn điều kiện).\nNhắc lại khái niệm \\(\\arg\\min\\): Chúng ta biết ký hiệu \\(\\min\\) là giá trị nhỏ nhất của hàm số, \\(\\arg\\min\\) chính là giá trị của biến số để hàm số đó đạt giá trị nhỏ nhất đó. Nếu \\(f(x) = x^2 -2x + 1 = (x-1)^2 \\) thì giá trị nhỏ nhất của hàm số này bằng 0, đạt được khi \\(x = 1\\). Trong ví dụ này \\(\\min_{x} f(x) = 0\\) và \\(\\arg\\min_{x} f(x) = 1\\). Thêm ví dụ khác, nếu \\(x_1 = 0, x_2 = 10, x_3 = 5\\) thì ta nói \\(\\arg\\min_{i} x_i = 1\\) vì \\(1\\) là chỉ số để \\(x_i\\) đạt giá trị nhỏ nhất (bằng \\(0\\)). Biến số viết bên dưới \\(\\min\\) là biến số cúng ta cần tối ưu. Trong các bài toán tối ưu, ta thường quan tâm tới \\(\\arg\\min\\) hơn là \\(\\min\\).\nThuật toán tối ưu hàm mất mát\nBài toán \\((2)\\) là một bài toán khó tìm điểm tối ưu vì nó có thêm các điều kiện ràng buộc. Bài toán này thuộc loại mix-integer programming (điều kiện biến là số nguyên) - là loại rất khó tìm nghiệm tối ưu toàn cục (global optimal point, tức nghiệm làm cho hàm mất mát đạt giá trị nhỏ nhất có thể). Tuy nhiên, trong một số trường hợp chúng ta vẫn có thể tìm được phương pháp để tìm được nghiệm gần đúng hoặc điểm cực tiểu. (Nếu chúng ta vẫn nhớ chương trình toán ôn thi đại học thì điểm cực tiểu chưa chắc đã phải là điểm làm cho hàm số đạt giá trị nhỏ nhất).\nMột cách đơn giản để giải bài toán \\((2)\\) là xen kẽ giải \\(\\mathbf{Y}\\) và \\( \\mathbf{M}\\) khi biến còn lại được cố định. Đây là một thuật toán lặp, cũng là kỹ thuật phổ biến khi giải bài toán tối ưu. Chúng ta sẽ lần lượt giải quyết hai bài toán sau đây:\nCố định \\(\\mathbf{M} \\), tìm \\(\\mathbf{Y}\\)\nGiả sử đã tìm được các centers, hãy tìm các label vector để hàm mất mát đạt giá trị nhỏ nhất. Điều này tương đương với việc tìm cluster cho mỗi điểm dữ liệu.\nKhi các centers là cố định, bài toán tìm label vector cho toàn bộ dữ liệu có thể được chia nhỏ thành bài toán tìm label vector cho từng điểm dữ liệu \\(\\mathbf{x}_i\\) như sau:\n\\[\n\\mathbf{y}_i = \\arg\\min_{\\mathbf{y}_i} \\sum_{j=1}^K y_{ij}\\|\\mathbf{x}_i - \\mathbf{m}_j\\|_2^2 ~~~ (3)\n\\]\n\\[\n\\text{subject to:} ~~ y_{ij} \\in \\{0, 1\\}~~ \\forall j;~~~ \\sum_{j = 1}^K y_{ij} = 1\n\\]\nVì chỉ có một phần tử của label vector \\(\\mathbf{y}_i\\) bằng \\(1\\) nên bài toán \\((3)\\) có thể tiếp tục được viết dưới dạng đơn giản hơn:\n\\[\nj = \\arg\\min_{j} \\|\\mathbf{x}_i - \\mathbf{m}_j\\|_2^2\n\\]\nVì \\(\\|\\mathbf{x}_i - \\mathbf{m}_j\\|_2^2\\) chính là bình phương khoảng cách tính từ điểm \\(\\mathbf{x}_i \\) tới center \\(\\mathbf{m}_j \\), ta có thể kết luận rằng mỗi điểm \\(\\mathbf{x}_i \\) thuộc vào cluster có center gần nó nhất! Từ đó ta có thể dễ dàng suy ra label vector của từng điểm dữ liệu.\nCố định \\(\\mathbf{Y} \\), tìm \\(\\mathbf{M}\\)\nGiả sử đã tìm được cluster cho từng điểm, hãy tìm center mới cho mỗi cluster để hàm mất mát đạt giá trị nhỏ nhất.\nMột khi chúng ta đã xác định được label vector cho từng điểm dữ liệu, bài toán tìm center cho mỗi cluster được rút gọn thành:\n\\[\n\\mathbf{m}_j = \\arg\\min_{\\mathbf{m}_j} \\sum_{i = 1}^{N} y_{ij}\\|\\mathbf{x}_i - \\mathbf{m}_j \\|_2^2.\n\\]\nTới đây, ta có thể tìm nghiệm bằng phương pháp giải đạo hàm bằng 0, vì hàm cần tối ưu là một hàm liên tục và có đạo hàm xác định tại mọi điểm. Và quan trọng hơn, hàm này là hàm convex (lồi) theo \\(\\mathbf{m}_j \\) nên chúng ta sẽ tìm được giá trị nhỏ nhất và điểm tối ưu tương ứng. Sau này nếu có dịp, tôi sẽ nói thêm về tối ưu lồi (convex optimization) - một mảng cực kỳ quan trọng trong toán tối ưu.\nĐặt \\(l(\\mathbf{m}_j)\\) là hàm bên trong dấu \\(\\arg\\min\\), ta có đạo hàm:\n\\[\n\\frac{\\partial l(\\mathbf{m}_j)}{\\partial \\mathbf{m}_j} = 2\\sum_{i=1}^N y_{ij}(\\mathbf{m}_j - \\mathbf{x}_i)\n\\]\nGiải phương trình đạo hàm bằng 0 ta có:\n\\[\n\\mathbf{m}_j \\sum_{i=1}^N y_{ij} = \\sum_{i=1}^N y_{ij} \\mathbf{x}_i\n\\]\n\\[\n\\Rightarrow \\mathbf{m}_j = \\frac{ \\sum_{i=1}^N y_{ij} \\mathbf{x}_i}{\\sum_{i=1}^N y_{ij}}\n\\]\nNếu để ý một chút, chúng ta sẽ thấy rằng mẫu số chính là phép đếm số lượng các điểm dữ liệu trong cluster \\(j\\) (Bạn có nhận ra không?). Còn tử số chính là tổng các điểm dữ liệu trong cluster \\(j\\). (Nếu bạn đọc vẫn nhớ điều kiện ràng buộc của các \\(y_{ij} \\) thì sẽ có thể nhanh chóng nhìn ra điều này).\nHay nói một cách đơn giản hơn nhiều: \\(\\mathbf{m}_j\\) là trung bình cộng của các điểm trong cluster \\(j\\).\nTên gọi K-means clustering cũng xuất phát từ đây.\nTóm tắt thuật toán\nTới đây tôi xin được tóm tắt lại thuật toán (đặc biệt quan trọng với các bạn bỏ qua phần toán học bên trên) như sau:\nĐầu vào: Dữ liệu \\(\\mathbf{X}\\) và số lượng cluster cần tìm \\(K\\).\nĐầu ra: Các center \\(\\mathbf{M}\\) và label vector cho từng điểm dữ liệu \\(\\mathbf{Y}\\).\nChọn \\(K\\) điểm bất kỳ làm các center ban đầu.\nPhân mỗi điểm dữ liệu vào cluster có center gần nó nhất.\nNếu việc gán dữ liệu vào từng cluster ở bước 2 không thay đổi so với vòng lặp trước nó thì ta dừng thuật toán.\nCập nhật center cho từng cluster bằng cách lấy trung bình cộng của tất các các điểm dữ liệu đã được gán vào cluster đó sau bước 2.\nQuay lại bước 2.\nChúng ta có thể đảm bảo rằng thuật toán sẽ dừng lại sau một số hữu hạn vòng lặp. Thật vậy, vì hàm mất mát là một số dương và sau mỗi bước 2 hoặc 3, giá trị của hàm mất mát bị giảm đi. Theo kiến thức về dãy số trong chương trình cấp 3: nếu một dãy số giảm và bị chặn dưới thì nó hội tụ! Hơn nữa, số lượng cách phân nhóm cho toàn bộ dữ liệu là hữu hạn nên đến một lúc nào đó, hàm mất mát sẽ không thể thay đổi, và chúng ta có thể dừng thuật toán tại đây.\nChúng ta sẽ có một vài thảo luận về thuật toán này, về những hạn chế và một số phương pháp khắc phục. Nhưng trước hết, hãy xem nó thể hiện như thế nào trong một ví dụ cụ thể dưới đây.\n3. Ví dụ trên Python\nGiới thiệu bài toán\nĐể kiểm tra mức độ hiểu quả của một thuật toán, chúng ta sẽ làm một ví dụ đơn giản (thường được gọi là toy example). Trước hết, chúng ta chọn center cho từng cluster và tạo dữ liệu cho từng cluster bằng cách lấy mẫu theo phân phối chuẩn có kỳ vọng là center của cluster đó và ma trận hiệp phương sai (covariance matrix) là ma trận đơn vị.\nTrước tiên, chúng ta cần khai báo các thư viện cần dùng. Chúng ta cần numpy và matplotlib như trong bài Linear Regression cho việc tính toán ma trận và hiển thị dữ liệu. Chúng ta cũng cần thêm thư viện scipy.spatial.distance để tính khoảng cách giữa các cặp điểm trong hai tập hợp một cách hiệu quả.\nfrom __future__ import print_function\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.spatial.distance import cdist\nnp.random.seed(11)\nTiếp theo, ta tạo dữ liệu bằng cách lấy các điểm theo phân phối chuẩn có kỳ vọng tại các điểm có tọa độ (2, 2), (8, 3) và (3, 6), ma trận hiệp phương sai giống nhau và là ma trận đơn vị. Mỗi cluster có 500 điểm. (Chú ý rằng mỗi điểm dữ liệu là một hàng của ma trận dữ liệu.\nmeans = [[2, 2], [8, 3], [3, 6]]\ncov = [[1, 0], [0, 1]]\nN = 500\nX0 = np.random.multivariate_normal(means[0], cov, N)\nX1 = np.random.multivariate_normal(means[1], cov, N)\nX2 = np.random.multivariate_normal(means[2], cov, N)\nX = np.concatenate((X0, X1, X2), axis = 0)\nK = 3\noriginal_label = np.asarray([0]*N + [1]*N + [2]*N).T\nHiển thị dữ liệu trên đồ thị\nChúng ta cần một hàm kmeans_display để hiển thị dữ liệu. Sau đó hiển thị dữ liệu theo nhãn ban đầu.\ndef kmeans_display(X, label):\nK = np.amax(label) + 1\nX0 = X[label == 0, :]\nX1 = X[label == 1, :]\nX2 = X[label == 2, :]\nplt.plot(X0[:, 0], X0[:, 1], 'b^', markersize = 4, alpha = .8)\nplt.plot(X1[:, 0], X1[:, 1], 'go', markersize = 4, alpha = .8)\nplt.plot(X2[:, 0], X2[:, 1], 'rs', markersize = 4, alpha = .8)\nplt.axis('equal')\nplt.plot()\nplt.show()\nkmeans_display(X, original_label)\nTrong đồ thị trên, mỗi cluster tương ứng với một màu. Có thể nhận thấy rằng có một vài điểm màu đỏ bị lẫn sang phần cluster màu xanh.\nCác hàm số cần thiết cho K-means clustering\nViết các hàm:\nkmeans_init_centers để khởi tạo các centers ban đầu.\nkmeans_asign_labels để gán nhán mới cho các điểm khi biết các centers.\nkmeans_update_centers để cập nhật các centers mới dữa trên dữ liệu vừa được gán nhãn.\nhas_converged để kiểm tra điều kiện dừng của thuật toán.\ndef kmeans_init_centers(X, k):\n# randomly pick k rows of X as initial centers\nreturn X[np.random.choice(X.shape[0], k, replace=False)]\ndef kmeans_assign_labels(X, centers):\n# calculate pairwise distances btw data and centers\nD = cdist(X, centers)\n# return index of the closest center\nreturn np.argmin(D, axis = 1)\ndef kmeans_update_centers(X, labels, K):\ncenters = np.zeros((K, X.shape[1]))\nfor k in range(K):\n# collect all points assigned to the k-th cluster\nXk = X[labels == k, :]\n# take average\ncenters[k,:] = np.mean(Xk, axis = 0)\nreturn centers\ndef has_converged(centers, new_centers):\n# return True if two sets of centers are the same\nreturn (set([tuple(a) for a in centers]) ==\nset([tuple(a) for a in new_centers]))\nPhần chính của K-means clustering:\ndef kmeans(X, K):\ncenters = [kmeans_init_centers(X, K)]\nlabels = []\nit = 0\nwhile True:\nlabels.append(kmeans_assign_labels(X, centers[-1]))\nnew_centers = kmeans_update_centers(X, labels[-1], K)\nif has_converged(centers[-1], new_centers):\nbreak\ncenters.append(new_centers)\nit += 1\nreturn (centers, labels, it)\nÁp dụng thuật toán vừa viết vào dữ liệu ban đầu, hiển thị kết quả cuối cùng.\n(centers, labels, it) = kmeans(X, K)\nprint('Centers found by our algorithm:')\nprint(centers[-1])\nkmeans_display(X, labels[-1])\nCenters found by our algorithm:\n[[ 1.97563391  2.01568065]\n[ 8.03643517  3.02468432]\n[ 2.99084705  6.04196062]]\nTừ kết quả này chúng ta thấy rằng thuật toán K-means clustering làm việc khá thành công, các centers tìm được khá gần với kỳ vọng ban đầu. Các điểm thuộc cùng một cluster hầu như được phân vào cùng một cluster (trừ một số điểm màu đỏ ban đầu đã bị phân nhầm vào cluster màu xanh da trời, nhưng tỉ lệ là nhỏ và có thể chấp nhận được).\nDưới đây là hình ảnh động minh họa thuật toán qua từng vòng lặp, chúng ta thấy rằng thuật toán trên hội tụ rất nhanh, chỉ cần 6 vòng lặp để có được kết quả cuối cùng:\nCác bạn có thể xem thêm các trang web minh họa thuật toán K-means cluster tại:\nVisualizing K-Means Clustering\nVisualizing K-Means Clustering - Standford\nKết quả tìm được bằng thư viện scikit-learn\nĐể kiểm tra thêm, chúng ta hãy so sánh kết quả trên với kết quả thu được bằng cách sử dụng thư viện scikit-learn.\nfrom sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=3, random_state=0).fit(X)\nprint('Centers found by scikit-learn:')\nprint(kmeans.cluster_centers_)\npred_label = kmeans.predict(X)\nkmeans_display(X, pred_label)\nCenters found by scikit-learn:\n[[ 8.0410628   3.02094748]\n[ 2.99357611  6.03605255]\n[ 1.97634981  2.01123694]]\nThật may mắn (cho tôi), hai thuật toán cho cùng một đáp số! Với cách thứ nhất, tôi mong muốn các bạn hiểu rõ được thuật toán K-means clustering làm việc như thế nào. Với cách thứ hai, tôi hy vọng các bạn biết áp dụng thư viện sẵn có như thế nào.\n4. Thảo luận\nHạn chế\nCó một vài hạn chế của thuật toán K-means clustering:\nChúng ta cần biết số lượng cluster cần clustering\nĐể ý thấy rằng trong thuật toán nêu trên, chúng ta cần biết đại lượng \\(K\\) là số lượng clusters. Trong thực tế, nhiều trường hợp chúng ta không xác định được giá trị này. Có một số phương pháp giúp xác định số lượng clusters, tôi sẽ dành thời gian nói về chúng sau nếu có dịp. Bạn đọc có thể tham khảo Elbow method - Determining the number of clusters in a data set.\nNghiệm cuối cùng phụ thuộc vào các centers được khởi tạo ban đầu\nTùy vào các center ban đầu mà thuật toán có thể có tốc độ hội tụ rất chậm, ví dụ:\nhoặc thậm chí cho chúng ta nghiệm không chính xác (chỉ là local minimum - điểm cực tiểu - mà không phải giá trị nhỏ nhất):\nCó một vài cách khắc phục đó là:\nChạy K-means clustering nhiều lần với các center ban đầu khác nhau rồi chọn cách có hàm mất mát cuối cùng đạt giá trị nhỏ nhất.\nK-means++ -Improve initialization algorithm - wiki.\nBạn nào muốn tìm hiểu sâu hơn có thể xem bài báo khoa học Cluster center initialization algorithm for K-means clustering.\nCác cluster cần có só lượng điểm gần bằng nhau\nDưới đây là một ví dụ với 3 cluster với 20, 50, và 1000 điểm. Kết quả cuối cùng không chính xác.\nCác cluster cần có dạng hình tròn\nTức các cluster tuân theo phân phối chuẩn và ma trận hiệp phương sai là ma trận đường chéo có các điểm trên đường chéo giống nhau.\nDưới đây là 1 ví dụ khi 1 cluster có dạng hình dẹt.\nKhi một cluster nằm phía trong 1 cluster khác\nĐây là ví dụ kinh điển về việc K-means clustering không thể phân cụm dữ liệu. Một cách tự nhiên, chúng ta sẽ phân ra thành 4 cụm: mắt trái, mắt phải, miệng, xung quanh mặt. Nhưng vì mắt và miệng nằm trong khuôn mặt nên K-means clustering không thực hiện được:\nMặc dù có những hạn chế, K-means clustering vẫn cực kỳ quan trọng trong Machine Learning và là nền tảng cho nhiều thuật toán phức tạp khác sau này. Chúng ta cần bắt đầu từ những thứ đơn giản. Simple is best!\n5. Tài liệu tham khảo\nClustering documents using k-means\nVoronoi Diagram - Wikipedia\nCluster center initialization algorithm for K-means clustering\nVisualizing K-Means Clustering\nVisualizing K-Means Clustering - Standford",
        "summary": "Thuật toán K-means clustering là một phương pháp phân cụm không giám sát, được sử dụng để phân chia dữ liệu thành các nhóm dựa trên sự gần gũi giữa các điểm dữ liệu. Thuật toán hoạt động bằng cách lặp đi lặp lại hai bước: gán nhãn cho các điểm dữ liệu dựa trên khoảng cách đến các trung tâm cụm và cập nhật vị trí các trung tâm cụm dựa trên các điểm dữ liệu đã được gán nhãn. Thuật toán K-means clustering có một số hạn chế, bao gồm việc cần xác định trước số lượng cụm, phụ thuộc vào vị trí ban đầu của các trung tâm cụm và hiệu quả kém khi các cụm có kích thước không đồng đều hoặc hình dạng không tròn. \n",
        "status": true
    }
}